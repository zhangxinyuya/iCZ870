{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# gapfilling\n",
   "id": "a9f6cbed660f40f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cobra\n",
    "import pandas as pd\n",
    "\n",
    "def split_reversible_reactions(model):\n",
    "        \"\"\"\n",
    "        Split reversible reactions in the model into separate forward and backward reactions.\n",
    "        \n",
    "        Parameters:\n",
    "        model (cobra.Model): The metabolic model.\n",
    "        \n",
    "        Returns:\n",
    "        cobra.Model: The model with split reversible reactions.\n",
    "        \"\"\"\n",
    "        for rxn in list(model.reactions):\n",
    "            if rxn.lower_bound < 0 and rxn.upper_bound > 0:\n",
    "                # Create forward and backward reactions\n",
    "                forward_rxn = cobra.Reaction(f\"{rxn.id}_forward\")\n",
    "                backward_rxn = cobra.Reaction(f\"{rxn.id}_backward\")\n",
    "                \n",
    "                # Add the new forward and backward reactions to the model\n",
    "                model.add_reactions([forward_rxn, backward_rxn])\n",
    "                \n",
    "                # Set the reaction attributes after adding to the model\n",
    "                forward_rxn.reaction = rxn.reaction\n",
    "                forward_rxn.lower_bound = 0\n",
    "                forward_rxn.upper_bound = rxn.upper_bound\n",
    "                \n",
    "                # Create the backward reaction using the reversed metabolites\n",
    "                backward_rxn.add_metabolites({k: -v for k, v in rxn.metabolites.items()})\n",
    "                backward_rxn.lower_bound = 0\n",
    "                backward_rxn.upper_bound = abs(rxn.lower_bound)\n",
    "                \n",
    "                # Remove the original reversible reaction\n",
    "                model.remove_reactions([rxn])\n",
    "                \n",
    "        return model\n",
    "def merge_and_gapfill(model, universal_model, target_met_id='biomass_c', delta=1, output_file='gapfilling_results.xlsx'):\n",
    "    \"\"\"\n",
    "    Merge a universal model with the given model and adjust weights for the universal model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (cobra.Model): The metabolic model to be gapfilled.\n",
    "    universal_model (cobra.Model): The universal metabolic model (e.g., ModelSEED).\n",
    "    target_met_id (str): The ID of the target metabolite to be gapfilled.\n",
    "    delta (float): The weight for minimizing the number of universal reactions.\n",
    "    output_file (str): The name of the output file to store the added reactions.\n",
    "    \n",
    "    Returns:\n",
    "    cobra.Model: The merged model with reactions added from the universal model.\n",
    "    \"\"\"\n",
    "    # Copy the original model\n",
    "    combined_model = model.copy()\n",
    "    combined_model.add_reactions(universal_model.reactions)\n",
    "    # Create a pseudo-reaction to represent the number of universal reactions\n",
    "    num_universal_rxn = cobra.Reaction('num_universal_rxn')\n",
    "    num_universal_rxn.lower_bound = 0\n",
    "    combined_model.add_reactions([num_universal_rxn])\n",
    "    \n",
    "    # Add the pseudo-metabolite to each universal reaction\n",
    "    num_universal_met = cobra.Metabolite('num_universal_met', formula='')\n",
    "    num_universal_met.compartment = 'c'\n",
    "    for rxn in combined_model.reactions:\n",
    "        if rxn.id not in model.reactions:\n",
    "            rxn.add_metabolites({num_universal_met: delta})\n",
    "            num_universal_rxn.add_metabolites({num_universal_met: -1})\n",
    "            #print(rxn)\n",
    "    added_reactions = []\n",
    "    \n",
    "    # Get the metabolite object for the target metabolite\n",
    "    target_met = model.metabolites.get_by_id(target_met_id)\n",
    "    print(f\"Processing metabolite {target_met_id}...\")\n",
    "\n",
    "    # Add a demand reaction for the target metabolite in the original model\n",
    "    demand_rxn_orig = cobra.Reaction(f'DM_{target_met_id}_orig')\n",
    "    demand_rxn_orig.add_metabolites({target_met: -1})\n",
    "    demand_rxn_orig.bounds = (0, 10)\n",
    "    model.add_reactions([demand_rxn_orig])\n",
    "    model.objective = demand_rxn_orig\n",
    "    orig_sol = model.optimize()\n",
    "    \n",
    "    # Add a demand reaction for the target metabolite in the combined model \n",
    "    demand_rxn_combined = cobra.Reaction(f'DM_{target_met_id}')\n",
    "    demand_rxn_combined.add_metabolites({target_met: -1})\n",
    "    demand_rxn_combined.bounds = (0,1000)\n",
    "    combined_model.add_reactions([demand_rxn_combined])\n",
    "    \n",
    "    # Set the objective to maximize the demand reaction and minimize the number of universal reactions\n",
    "    combined_model.objective = {demand_rxn_combined: 1, num_universal_rxn: -delta}# \n",
    "    \n",
    "    # Perform optimization on the combined model\n",
    "    combined_sol = combined_model.optimize()\n",
    "    print(f\"Original model flux: {orig_sol.objective_value}, Combined model flux: {combined_sol.fluxes[demand_rxn_combined.id]}\")\n",
    "    print(f\"Number of universal reactions used: {combined_sol.fluxes[num_universal_rxn.id]}\")\n",
    "    \n",
    "    if orig_sol.objective_value < 1e-6:\n",
    "        print(f'Metabolite {target_met_id} cannot be synthesized in the original model.')\n",
    "        \n",
    "        if combined_sol.fluxes[demand_rxn_combined.id] >= 1e-6:\n",
    "            print(f'Metabolite {target_met_id} can be synthesized after gapfilling.')                     \n",
    "            # Extract active reactions from the universal model that contribute to gapfilling\n",
    "            active_universal_rxns = [\n",
    "                rxn for rxn in universal_model.reactions\n",
    "                if rxn.id in combined_sol.fluxes and abs(float(combined_sol.fluxes[rxn.id])) > 1e-6\n",
    "            ]\n",
    "            \n",
    "            model.add_reactions(active_universal_rxns)\n",
    "            # Record added reactions for output \n",
    "            for rxn in active_universal_rxns:\n",
    "                added_reactions.append({\n",
    "                    'Metabolite': target_met_id,\n",
    "                    'Reaction ID': rxn.id,\n",
    "                    'Reaction Name': rxn.name,\n",
    "                    'Reaction Formula': str(rxn.reaction),\n",
    "                    'Lower Bound': rxn.lower_bound,\n",
    "                    'Upper Bound': rxn.upper_bound,\n",
    "                    'Flux Value': combined_sol.fluxes[rxn.id]\n",
    "                })\n",
    "                \n",
    "            print(f'Identified {len(active_universal_rxns)} reactions from universal model for {target_met_id}.')\n",
    "        else:\n",
    "            print(f'Metabolite {target_met_id} cannot be synthesized even after gapfilling.') \n",
    "    else:\n",
    "        print(f'Metabolite {target_met_id} can be synthesized in the original model.')\n",
    "\n",
    "    # Remove the demand reactions from both models\n",
    "    model.remove_reactions([demand_rxn_orig]) \n",
    "    combined_model.remove_reactions([demand_rxn_combined])\n",
    "\n",
    "    # Remove the pseudo-reaction for number of universal reactions    \n",
    "    combined_model.remove_reactions([num_universal_rxn])\n",
    "    \n",
    "    # Create a DataFrame of added reactions for output\n",
    "    df_added_reactions = pd.DataFrame(added_reactions)\n",
    "\n",
    "    # Write the added reactions to an Excel file\n",
    "    df_added_reactions.to_excel(output_file, index=False) \n",
    "    print(f\"Added reactions saved to {output_file}\")\n",
    "\n",
    "    return model\n",
    "model = cobra.io.read_sbml_model('14067gem13.xml')  # Your metabolic model  \n",
    "universal_model = cobra.io.load_json_model('5. GAPFILLING/processed_universal_modelseed.json')  # Your universal model\n",
    "# Split reversible reactions in the universal model\n",
    "universal_model_split = split_reversible_reactions(universal_model.copy())\n",
    "gapfilled_model = merge_and_gapfill(model, universal_model_split, target_met_id='biomass_c')\n",
    "# Save the gapfilled model\n",
    "cobra.io.write_sbml_model(gapfilled_model, '14067gem13_gap.xml') \n",
    "print('Gapfilling complete for biomass_c. Gapfilled model saved as gapfilled_model_biomass.xml')\n",
    "# Perform FBA on the gapfilled model\n",
    "gapfilled_model.objective = gapfilled_model.reactions.get_by_id('Bio_cgATCC14067_c') \n",
    "fba_solution = gapfilled_model.optimize()\n",
    "if fba_solution.objective_value >= 1e-6:\n",
    "    print('Gapfilled model can produce biomass.')\n",
    "else:\n",
    "    print('Gapfilled model cannot produce biomass. Please check the model.')"
   ],
   "id": "1f3ca5308b831c69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cbae0cec3aa8be3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 去除重复值",
   "id": "45058214e5fa4b04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 Excel 文件\n",
    "df = pd.read_excel('gapfilling_results.xlsx')\n",
    "\n",
    "# 根据 \"Reaction ID\" 列删除重复值所在的行\n",
    "df.drop_duplicates(subset='Reaction ID', inplace=True)\n",
    "\n",
    "# 将结果保存到新的 Excel 文件\n",
    "df.to_excel('gapfilling_results.xlsx', index=False)"
   ],
   "id": "f59083f923622035",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 去除已在模型中的反应",
   "id": "385e17e82ca01d78"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取两个xlsx文件\n",
    "df1 = pd.read_excel('trans.xlsx')  \n",
    "df2 = pd.read_excel('14067gem13.xlsx')\n",
    "\n",
    "# 获取df1中\"Reaction ID\"列的值\n",
    "reaction_ids = df1['Reaction ID'].tolist()\n",
    "\n",
    "# 获取df2中\"ID\"列的值 \n",
    "ids = df2['ID'].tolist()\n",
    "\n",
    "# 找出reaction_ids中不存在于ids的值\n",
    "missing_ids = [id for id in reaction_ids if id not in ids]\n",
    "\n",
    "# 根据missing_ids从df1中筛选出相应的行\n",
    "missing_rows = df1[df1['Reaction ID'].isin(missing_ids)]\n",
    "\n",
    "# 将筛选出的行写入新的xlsx文件\n",
    "missing_rows.to_excel('gapfilling_results.xlsx', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 修正反应式",
   "id": "353acb11df9123b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_files(xlsx_path, tsv_path, output_path):\n",
    "    # 读取Excel文件\n",
    "    df_excel = pd.read_excel(xlsx_path)\n",
    "    \n",
    "    # 读取TSV文件，使用tab作为分隔符\n",
    "    df_tsv = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # 创建一个函数来处理Reaction ID\n",
    "    def process_reaction_id(reaction_id):\n",
    "        if pd.isna(reaction_id):\n",
    "            return reaction_id\n",
    "        # 使用正则表达式匹配rxn后面的5位数字\n",
    "        match = re.search(r'rxn(\\d{5})', reaction_id)\n",
    "        if match:\n",
    "            # 提取5位数字并构建新的ID格式\n",
    "            number = match.group(1)\n",
    "            return f'rxn{number}_c0'\n",
    "        return reaction_id\n",
    "\n",
    "    # 处理Reaction ID列\n",
    "    df_excel['Reaction ID'] = df_excel['Reaction ID'].apply(process_reaction_id)\n",
    "    \n",
    "    # 创建一个映射字典，用于快速查找\n",
    "    tsv_mapping = dict(zip(df_tsv['id'], zip(df_tsv['name'], df_tsv['equation'], df_tsv['definition'])))\n",
    "    \n",
    "    # 创建用于存储匹配结果的新列\n",
    "    df_excel['Reaction Name'] = ''\n",
    "    df_excel['Reaction Formula'] = ''\n",
    "    df_excel['Definition'] = ''  # 新建列用于存储definition\n",
    "    \n",
    "    # 遍历Excel文件中的每一行\n",
    "    for index, row in df_excel.iterrows():\n",
    "        reaction_id = row['Reaction ID']\n",
    "        if pd.notna(reaction_id):\n",
    "            # 从reaction_id中提取5位数字部分\n",
    "            match = re.search(r'rxn(\\d{5})', reaction_id)\n",
    "            if match:\n",
    "                search_id = f'rxn{match.group(1)}'\n",
    "                # 在TSV数据中查找匹配项\n",
    "                if search_id in tsv_mapping:\n",
    "                    name, equation, definition = tsv_mapping[search_id]\n",
    "                    df_excel.at[index, 'Reaction Name'] = name\n",
    "                    df_excel.at[index, 'Reaction Formula'] = equation\n",
    "                    df_excel.at[index, 'Definition'] = definition\n",
    "    \n",
    "    # 保存处理后的Excel文件\n",
    "    df_excel.to_excel(output_path, index=False)\n",
    "    print(f'处理完成，文件已保存至: {output_path}')\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    xlsx_path = \"111111.xlsx\"  # 替换为你的Excel文件路径\n",
    "    tsv_path = \"4. add linker/modelseed_reactions.tsv\"  # TSV文件路径\n",
    "    output_path = \"r1.xlsx\"  # 输出文件路径\n",
    "    \n",
    "    process_files(xlsx_path, tsv_path, output_path)"
   ],
   "id": "9d091edc871cc9b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def reverse_reaction(reaction):\n",
    "    if not isinstance(reaction, str):  # 检查是否为字符串\n",
    "        return reaction\n",
    "    \n",
    "    if '<=' in reaction and '<=>' not in reaction:\n",
    "        # 分割 '<=' 并反转方向\n",
    "        reaction = reaction.replace('[0]', '_c')\n",
    "        reaction = reaction.replace('[1]', '_e')\n",
    "        products, reactants = reaction.split('<=')\n",
    "        # 去除多余空格并拼接成标准化形式\n",
    "        \n",
    "        return f\"{reactants.strip()} => {products.strip()}\"\n",
    "        \n",
    "    # 替换 [0] 为 _c 和 [1] 为 _e\n",
    "    reaction = reaction.replace('[0]', '_c')\n",
    "    reaction = reaction.replace('[1]', '_e')\n",
    "    \n",
    "    return reaction\n",
    "\n",
    "def process_excel(input_file, output_file):\n",
    "    try:\n",
    "        # 创建输出目录（如果不存在）\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # 读取Excel文件\n",
    "        print(f\"正在读取文件: {input_file}\")\n",
    "        df = pd.read_excel(input_file)\n",
    "        \n",
    "        # 处理Definition列的反应\n",
    "        print(\"正在处理反应方程...\")\n",
    "        df['Definition'] = df['Definition'].apply(reverse_reaction)\n",
    "        df['Reaction Formula'] = df['Reaction Formula'].apply(reverse_reaction)\n",
    "        # 保存处理后的文件\n",
    "        print(f\"正在保存结果到: {output_file}\")\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(\"处理完成！\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理过程中发生错误: {str(e)}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"r1.xlsx\"\n",
    "    output_file = \"r2.xlsx\"\n",
    "    \n",
    "    process_excel(input_file, output_file)"
   ],
   "id": "e53543c5bd29a2b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 添加bigg linker",
   "id": "ce3ce4ec032acd59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取数据\n",
    "xlsx_file = 'r2.xlsx'  # 这里替换为你的xlsx文件路径\n",
    "tsv_file = '4. add linker/modelseed_reactions.tsv'  # 这里替换为你的tsv文件路径\n",
    "# 读取 Excel 和 TSV 文件\n",
    "df_xlsx = pd.read_excel(xlsx_file)\n",
    "df_tsv = pd.read_csv(tsv_file, sep='\\t')\n",
    "\n",
    "# 添加 seed.reaction 列：从 ID 列提取 rxn{5}\n",
    "df_xlsx['seed.reaction'] = df_xlsx['Reaction ID'].str.extract(r'(rxn\\d{5})')\n",
    "\n",
    "# 函数：从 aliases 列提取 BiGG 和 KEGG 信息（保留多个）\n",
    "def extract_bigg_kegg(aliases):\n",
    "    # 确保 aliases 是字符串类型\n",
    "    if isinstance(aliases, str):\n",
    "        # 提取所有 BiGG 名称\n",
    "        bigg_matches = re.findall(r'BiGG: ([^|]+)', aliases)\n",
    "        bigg = '; '.join(bigg_matches) if bigg_matches else ''\n",
    "        \n",
    "        # 提取所有 KEGG 编号\n",
    "        kegg_matches = re.findall(r'KEGG: ([^|]+)', aliases)\n",
    "        kegg = '; '.join(kegg_matches) if kegg_matches else ''\n",
    "        \n",
    "        return bigg, kegg\n",
    "    return '', ''  # 如果 aliases 不是字符串，返回空值\n",
    "\n",
    "# 函数：获取 ec_numbers，只保留第一个 EC 编号\n",
    "def extract_ec_numbers(ec_numbers):\n",
    "    if pd.notnull(ec_numbers):\n",
    "        # 如果包含多个 EC 编号，则只保留第一个（以 \"|\" 分隔）\n",
    "        ec_list = ec_numbers.split('|')\n",
    "        return ec_list[0]\n",
    "    return ''  # 如果为空，则返回空字符串\n",
    "\n",
    "# 创建 BiGG.reaction, KEGG.reaction, ec-code 和 definition 列\n",
    "df_xlsx['BiGG.reaction'] = ''\n",
    "df_xlsx['KEGG.reaction'] = ''\n",
    "df_xlsx['ec-code'] = ''\n",
    "\n",
    "\n",
    "# 遍历 Excel 文件中的每一行，进行匹配并更新列\n",
    "for idx, row in df_xlsx.iterrows():\n",
    "    seed_reaction = row['seed.reaction']\n",
    "    \n",
    "    # 根据 seed.reaction 去匹配 tsv 文件\n",
    "    matched_row = df_tsv[df_tsv['id'] == seed_reaction]\n",
    "    if not matched_row.empty:\n",
    "        # 提取 aliases, ec_numbers 和 definition\n",
    "        aliases = matched_row.iloc[0]['aliases']\n",
    "        ec_numbers = matched_row.iloc[0]['ec_numbers']\n",
    "        \n",
    "        \n",
    "        # 提取 BiGG 和 KEGG 信息\n",
    "        bigg, kegg = extract_bigg_kegg(aliases)\n",
    "        \n",
    "        # 更新 xlsx 数据框\n",
    "        df_xlsx.at[idx, 'BiGG.reaction'] = bigg\n",
    "        df_xlsx.at[idx, 'KEGG.reaction'] = kegg\n",
    "        df_xlsx.at[idx, 'ec-code'] = extract_ec_numbers(ec_numbers)\n",
    "        \n",
    "\n",
    "# 保存修改后的文件\n",
    "df_xlsx.to_excel('r3.xlsx', index=False)\n",
    "\n",
    "print(\"操作完成！\")"
   ],
   "id": "cd0ddfc5993d6ba8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_files(xlsx_path, tsv_path, output_path):\n",
    "    \"\"\"\n",
    "    处理xlsx和tsv文件，根据匹配规则更新BiGG.reaction列\n",
    "    支持多个bigg_id的情况，用分号分隔\n",
    "    \n",
    "    Parameters:\n",
    "    xlsx_path (str): 输入xlsx文件路径\n",
    "    tsv_path (str): 输入tsv文件路径\n",
    "    output_path (str): 输出xlsx文件路径\n",
    "    \"\"\"\n",
    "    # 读取xlsx文件\n",
    "    xlsx_df = pd.read_excel(xlsx_path)\n",
    "    \n",
    "    # 读取tsv文件\n",
    "    tsv_df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # 获取需要查找的rxn模式\n",
    "    rxn_pattern = re.compile(r'rxn\\d{5}_c0')\n",
    "    rxn_ids = []\n",
    "    \n",
    "    # 从xlsx文件中提取符合模式的ID\n",
    "    for idx, row in xlsx_df.iterrows():\n",
    "        if pd.isna(row['BiGG.reaction']):  # 检查BiGG.reaction是否为空\n",
    "            match = rxn_pattern.search(str(row['Reaction ID']))\n",
    "            if match:\n",
    "                rxn_ids.append((idx, match.group(0)[:8]))  # 只取rxn部分，不含_c0\n",
    "    \n",
    "    # 在tsv文件中查找对应的bigg_id\n",
    "    for idx, rxn_id in rxn_ids:\n",
    "        # 构造搜索模式\n",
    "        search_pattern = f\"SEED Reaction: http://identifiers.org/seed.reaction/{rxn_id}\"\n",
    "        \n",
    "        # 在database_links列中查找所有匹配项\n",
    "        matches = tsv_df[tsv_df['database_links'].str.contains(search_pattern, na=False)]\n",
    "        \n",
    "        if not matches.empty:\n",
    "            # 获取所有匹配的bigg_id并用分号连接\n",
    "            bigg_ids = matches['bigg_id'].unique()  # 使用unique()去除重复值\n",
    "            bigg_ids_str = '; '.join(bigg_ids)\n",
    "            \n",
    "            # 更新到xlsx文件中\n",
    "            xlsx_df.at[idx, 'BiGG.reaction'] = bigg_ids_str\n",
    "            \n",
    "            # 打印匹配信息（可选，用于调试）\n",
    "            print(f\"Found matches for {rxn_id}: {bigg_ids_str}\")\n",
    "    \n",
    "    # 保存更新后的xlsx文件\n",
    "    xlsx_df.to_excel(output_path, index=False)\n",
    "    print(f\"处理完成，结果已保存到: {output_path}\")\n",
    "    \n",
    "    # 打印统计信息\n",
    "    total_processed = len(rxn_ids)\n",
    "    total_matched = len(xlsx_df[xlsx_df['BiGG.reaction'].notna()])\n",
    "    print(f\"\\n统计信息:\")\n",
    "    print(f\"处理的rxn数量: {total_processed}\")\n",
    "    print(f\"成功匹配的数量: {total_matched}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 替换为实际的文件路径\n",
    "    xlsx_file = \"r3.xlsx\"\n",
    "    tsv_file = \"4. add linker/bigg_models_reactions.tsv\"\n",
    "    output_file = \"r4.xlsx\"\n",
    "    \n",
    "    process_files(xlsx_file, tsv_file, output_file)"
   ],
   "id": "ccff901dd3c8f36b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 添加反应其他linker",
   "id": "7613956391f50a26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_last_rhea(database_links):\n",
    "    \"\"\"提取最后一个RHEA ID\"\"\"\n",
    "    if pd.isna(database_links):\n",
    "        return None\n",
    "    \n",
    "    rhea_pattern = r'RHEA: http://identifiers\\.org/rhea/(\\d+)'\n",
    "    matches = re.finditer(rhea_pattern, database_links)\n",
    "    \n",
    "    # 转换迭代器到列表并获取最后一个匹配\n",
    "    matches_list = list(matches)\n",
    "    if matches_list:\n",
    "        return matches_list[-1].group(1)\n",
    "    return None\n",
    "\n",
    "def extract_metanetx(database_links):\n",
    "    \"\"\"提取MetaNetX reaction ID\"\"\"\n",
    "    if pd.isna(database_links):\n",
    "        return None\n",
    "    \n",
    "    metanetx_pattern = r'MetaNetX \\(MNX\\) Equation: http://identifiers\\.org/metanetx\\.reaction/(MNXR\\d+)'\n",
    "    match = re.search(metanetx_pattern, database_links)\n",
    "    \n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extract_biocyc(database_links):\n",
    "    \"\"\"提取BioCyc ID\"\"\"\n",
    "    if pd.isna(database_links):\n",
    "        return None\n",
    "    \n",
    "    biocyc_pattern = r'BioCyc: http://identifiers\\.org/biocyc/(META:[^;]+)'\n",
    "    match = re.search(biocyc_pattern, database_links)\n",
    "    \n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def process_files(xlsx_path, tsv_path, output_path):\n",
    "    \"\"\"\n",
    "    处理xlsx和tsv文件，提取相关ID并创建新列\n",
    "    \n",
    "    Parameters:\n",
    "    xlsx_path (str): 输入xlsx文件路径\n",
    "    tsv_path (str): 输入tsv文件路径\n",
    "    output_path (str): 输出xlsx文件路径\n",
    "    \"\"\"\n",
    "    # 读取文件\n",
    "    xlsx_df = pd.read_excel(xlsx_path)\n",
    "    tsv_df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # 初始化新列\n",
    "    xlsx_df['rhea'] = None\n",
    "    xlsx_df['MetanetX.reaction'] = None\n",
    "    xlsx_df['biocyc'] = None\n",
    "    \n",
    "    # 处理每一行\n",
    "    for idx, row in xlsx_df.iterrows():\n",
    "        bigg_reaction = row['BiGG.reaction']\n",
    "        \n",
    "        # 只处理非空的BiGG.reaction\n",
    "        if pd.notna(bigg_reaction):\n",
    "            # 在tsv文件中查找匹配的行\n",
    "            matches = tsv_df[tsv_df['bigg_id'] == bigg_reaction]\n",
    "            \n",
    "            if not matches.empty:\n",
    "                # 获取第一个匹配行的database_links\n",
    "                database_links = matches.iloc[0]['database_links']\n",
    "                \n",
    "                # 提取各种ID\n",
    "                rhea_id = extract_last_rhea(database_links)\n",
    "                metanetx_id = extract_metanetx(database_links)\n",
    "                biocyc_id = extract_biocyc(database_links)\n",
    "                \n",
    "                # 更新DataFrame\n",
    "                xlsx_df.at[idx, 'rhea'] = rhea_id\n",
    "                xlsx_df.at[idx, 'MetanetX.reaction'] = metanetx_id\n",
    "                xlsx_df.at[idx, 'biocyc'] = biocyc_id\n",
    "    \n",
    "    # 保存结果\n",
    "    xlsx_df.to_excel(output_path, index=False)\n",
    "    print(f\"处理完成，结果已保存到: {output_path}\")\n",
    "    \n",
    "    # 打印统计信息\n",
    "    total_processed = xlsx_df['BiGG.reaction'].notna().sum()\n",
    "    rhea_matched = xlsx_df['rhea'].notna().sum()\n",
    "    metanetx_matched = xlsx_df['MetanetX.reaction'].notna().sum()\n",
    "    biocyc_matched = xlsx_df['biocyc'].notna().sum()\n",
    "    \n",
    "    print(f\"\\n统计信息:\")\n",
    "    print(f\"处理的BiGG.reaction数量: {total_processed}\")\n",
    "    print(f\"成功匹配RHEA ID数量: {rhea_matched}\")\n",
    "    print(f\"成功匹配MetaNetX ID数量: {metanetx_matched}\")\n",
    "    print(f\"成功匹配BioCyc ID数量: {biocyc_matched}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 替换为实际的文件路径\n",
    "    xlsx_file = \"r4.xlsx\"\n",
    "    tsv_file = \"4. add linker/bigg_models_reactions.tsv\"\n",
    "    output_file = \"r5.xlsx\"\n",
    "    \n",
    "    process_files(xlsx_file, tsv_file, output_file)"
   ],
   "id": "9900906c04e71a56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 提取代谢物",
   "id": "f3579a50b117f801"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_metabolites(equation):\n",
    "    \"\"\"从反应方程式中提取所有代谢物（包括compartment标记）\"\"\"\n",
    "    if pd.isna(equation):\n",
    "        return []\n",
    "    \n",
    "    # 更新的正则表达式模式，匹配:\n",
    "    # 1. 带系数的代谢物: (数字) 代谢物[compartment]\n",
    "    # 2. 不带系数的代谢物: 代谢物[compartment]\n",
    "    pattern = r'([\\w\\-]+)_([ce]\\d*)'\n",
    "    \n",
    "    # 找出所有匹配项\n",
    "    matches = re.finditer(pattern, equation)\n",
    "    \n",
    "    # 提取代谢物名称和compartment\n",
    "    metabolites = [f\"{match.group(1)}_{match.group(2)}\" for match in matches]\n",
    "    \n",
    "    return metabolites\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    \"\"\"\n",
    "    处理Excel文件，提取并去重代谢物\n",
    "    \n",
    "    Parameters:\n",
    "    input_path (str): 输入xlsx文件路径\n",
    "    output_path (str): 输出xlsx文件路径\n",
    "    \"\"\"\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(input_path)\n",
    "    \n",
    "    # 存储所有唯一的代谢物\n",
    "    all_metabolites = set()\n",
    "    \n",
    "    # 从每个方程式中提取代谢物\n",
    "    for equation in df['Reaction Formula']:\n",
    "        metabolites = extract_metabolites(equation)\n",
    "        all_metabolites.update(metabolites)\n",
    "    \n",
    "    # 转换为排序后的列表\n",
    "    sorted_metabolites = sorted(list(all_metabolites))\n",
    "    \n",
    "    # 创建新的DataFrame\n",
    "    output_df = pd.DataFrame({'metabolites_id': sorted_metabolites})\n",
    "    \n",
    "    # 保存到新的Excel文件\n",
    "    output_df.to_excel(output_path, index=False)\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(f\"处理完成，结果已保存到: {output_path}\")\n",
    "    print(f\"找到的唯一代谢物数量: {len(sorted_metabolites)}\")\n",
    "    \n",
    "    # 打印一些示例进行验证\n",
    "    print(\"\\n代谢物示例:\")\n",
    "    # 打印部分带[c0]的代谢物\n",
    "    c0_mets = [m for m in sorted_metabolites if '_c' in m][:3]\n",
    "    print(\"带[c0]的代谢物示例:\", c0_mets)\n",
    "    \n",
    "    # 打印部分带[e0]的代谢物\n",
    "    e0_mets = [m for m in sorted_metabolites if '_e' in m][:3]\n",
    "    print(\"带[e0]的代谢物示例:\", e0_mets)\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"r5.xlsx\"\n",
    "    output_file = \"m1.xlsx\"\n",
    "    \n",
    "    process_file(input_file, output_file)"
   ],
   "id": "16f1abe53b628da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取xlsx文件\n",
    "df_xlsx = pd.read_excel('m1.xlsx')\n",
    "\n",
    "# 确保必要的列存在并填充空值\n",
    "columns_to_create = ['name', 'abbreviation', 'formula', 'mass', 'inchikey', 'charge', 'deltag', 'deltagerr', 'pka', 'pkb', 'smiles']\n",
    "for col in columns_to_create:\n",
    "    if col not in df_xlsx.columns:\n",
    "        df_xlsx[col] = pd.NA  # 创建列并填充NaN\n",
    "\n",
    "# 读取tsv文件\n",
    "df_tsv = pd.read_csv('4. add linker/modelseed_compounds.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "# 定义一个函数，用于匹配metabolite ID并从TSV中获取相应的数据\n",
    "def get_metabolite_data(metabolite_id, name):\n",
    "    stripped_id = re.sub(r'_[ce]$', '', str(metabolite_id))  # 清除掉cell状态\n",
    "    matched_compound = df_tsv[df_tsv['id'] == stripped_id]\n",
    "    \n",
    "    if not matched_compound.empty:\n",
    "        return matched_compound[['name', 'abbreviation', 'formula', 'mass', 'inchikey', 'charge', 'deltag', 'deltagerr', 'pka', 'pkb', 'smiles']].iloc[0]\n",
    "    \n",
    "    # 如果没有匹配的数据，则返回默认空值\n",
    "    return pd.Series(['']*11, index=['name', 'abbreviation', 'formula', 'mass', 'inchikey', 'charge', 'deltag', 'deltagerr', 'pka', 'pkb', 'smiles'])\n",
    "\n",
    "# 处理xlsx数据并添加匹配的tsv数据\n",
    "metabolite_data = df_xlsx.apply(lambda x: get_metabolite_data(x['metabolites_id'], x['name']), axis=1)\n",
    "\n",
    "# 更新xlsx中的列，如果tsv中有数据就更新，没有则保留原值或填充空值\n",
    "df_xlsx['name'] = df_xlsx['name'].where(pd.notnull(df_xlsx['name']), metabolite_data['name'])\n",
    "df_xlsx['abbreviation'] = df_xlsx['abbreviation'].where(pd.notnull(df_xlsx['abbreviation']), metabolite_data['abbreviation'])\n",
    "df_xlsx['formula'] = df_xlsx['formula'].where(pd.notnull(df_xlsx['formula']), metabolite_data['formula'])\n",
    "df_xlsx['mass'] = df_xlsx['mass'].where(pd.notnull(df_xlsx['mass']), metabolite_data['mass'])\n",
    "df_xlsx['charge'] = df_xlsx['charge'].where(pd.notnull(df_xlsx['charge']), metabolite_data['charge'])\n",
    "df_xlsx['pka'] = df_xlsx['pka'].where(pd.notnull(df_xlsx['pka']), metabolite_data['pka'])\n",
    "df_xlsx['pkb'] = df_xlsx['pkb'].where(pd.notnull(df_xlsx['pkb']), metabolite_data['pkb'])\n",
    "df_xlsx['smiles'] = df_xlsx['smiles'].where(pd.notnull(df_xlsx['smiles']), metabolite_data['smiles'])\n",
    "\n",
    "df_xlsx['inchikey'] = metabolite_data['inchikey']\n",
    "df_xlsx['deltaG'] = metabolite_data.apply(lambda x: f\"{x['deltag']}±{x['deltagerr']} (kcal/mol)\" if pd.notnull(x['deltag']) else '', axis=1)\n",
    "\n",
    "# 将更新后的数据写入新的xlsx文件\n",
    "df_xlsx.to_excel('m2.xlsx', index=False)\n",
    "print('met_pro2.xlsx 生成成功!')\n"
   ],
   "id": "15cd538b96587cdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_excel('m2.xlsx')\n",
    "\n",
    "# 读取TSV文件，设置low_memory=False来避免混合类型警告\n",
    "modelseed_df = pd.read_csv('4. add linker/modelseed_compounds.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "# 创建新列seed.compou，提取cpd编号\n",
    "df['seed.compound'] = df['metabolites_id'].str.extract(r'(cpd\\d+)')\n",
    "\n",
    "# 创建新列用于存储BiGG和KEGG的标识符\n",
    "df['BiGG.compound'] = ''\n",
    "df['KEGG.compound'] = ''\n",
    "\n",
    "# 遍历每一行\n",
    "for index, row in df.iterrows():\n",
    "    # 获取cpd编号\n",
    "    cpd = row['seed.compound']\n",
    "    if pd.notna(cpd):\n",
    "        # 在modelseed_compounds中查找匹配的行\n",
    "        match = modelseed_df[modelseed_df['id'] == cpd]\n",
    "        \n",
    "        if not match.empty:\n",
    "            aliases = match.iloc[0]['aliases']\n",
    "            \n",
    "            # 检查aliases是否为有效字符串\n",
    "            if isinstance(aliases, str):\n",
    "                # 提取BiGG标识符\n",
    "                bigg_match = re.search(r'BiGG: ([^|]+)', aliases)\n",
    "                if bigg_match:\n",
    "                    df.at[index, 'BiGG.compound'] = bigg_match.group(1)\n",
    "                \n",
    "                # 提取KEGG标识符\n",
    "                kegg_match = re.search(r'KEGG: ([^|]+)', aliases)\n",
    "                if kegg_match:\n",
    "                    df.at[index, 'KEGG.compound'] = kegg_match.group(1)\n",
    "\n",
    "# 保存结果到新的Excel文件\n",
    "df.to_excel('m3.xlsx', index=False)"
   ],
   "id": "20e2b7818ec9d6df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取Excel文件和TSV文件\n",
    "df = pd.read_excel('m3.xlsx')\n",
    "bigg_df = pd.read_csv('4. add linker/bigg_models_metabolites.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "# 创建新列\n",
    "df['MetaNetX.compound'] = ''\n",
    "df['Biocyc'] = ''\n",
    "\n",
    "# 遍历每一行\n",
    "for index, row in df.iterrows():\n",
    "    bigg_id = row['BiGG.compound']\n",
    "    \n",
    "    # 只处理BiGG.compound非空的行\n",
    "    if pd.notna(bigg_id):\n",
    "        # 在bigg_models_metabolites中查找匹配的行\n",
    "        match = bigg_df[bigg_df['universal_bigg_id'] == bigg_id]\n",
    "        \n",
    "        if not match.empty:\n",
    "            database_links = match.iloc[0]['database_links']\n",
    "            \n",
    "            # 确保database_links是字符串\n",
    "            if isinstance(database_links, str):\n",
    "                # 提取MetaNetX ID\n",
    "                metanetx_match = re.search(r'MetaNetX \\(MNX\\) Chemical: http://identifiers\\.org/metanetx\\.chemical/([^;]+)', database_links)\n",
    "                if metanetx_match:\n",
    "                    df.at[index, 'MetaNetX.compound'] = metanetx_match.group(1).strip()\n",
    "                \n",
    "                # 提取BioCyc ID\n",
    "                biocyc_match = re.search(r'BioCyc: http://identifiers\\.org/biocyc/([^;]+)', database_links)\n",
    "                if biocyc_match:\n",
    "                    df.at[index, 'Biocyc'] = biocyc_match.group(1).strip()\n",
    "\n",
    "# 保存结果到新的Excel文件\n",
    "df.to_excel('m4.xlsx', index=False)"
   ],
   "id": "3142f723f7bd65a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 met_pro4.xlsx 文件的 metabolites 工作簿\n",
    "df_met_pro4 = pd.read_excel('m4.xlsx')\n",
    "\n",
    "# 读取 14067gem2.xlsx 文件的 metabolites 工作簿\n",
    "df_14067gem2 = pd.read_excel('14067gem14.xlsx', sheet_name='metabolites')\n",
    "\n",
    "# 找到 met_pro4.xlsx 中存在而 14067gem2.xlsx 中不存在的代谢物所在的行\n",
    "new_metabolites = df_met_pro4[~df_met_pro4['metabolites_id'].isin(df_14067gem2['metabolites_id'])]\n",
    "\n",
    "# 将新的代谢物行添加到 14067gem2.xlsx 的 metabolites 工作簿中\n",
    "df_14067gem3 = pd.concat([df_14067gem2, new_metabolites], ignore_index=True)\n",
    "\n",
    "# 读取 14067gem2.xlsx 文件的其他工作簿\n",
    "xlsx_14067gem2 = pd.read_excel('14067gem14.xlsx', sheet_name=None)\n",
    "\n",
    "writer = pd.ExcelWriter('model_history/14067gem14.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# 将 14067gem2.xlsx 的其他工作簿写入新文件\n",
    "for sheet_name, df in xlsx_14067gem2.items():\n",
    "    if sheet_name != 'metabolites':\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# 将更新后的 metabolites 工作簿写入新文件\n",
    "df_14067gem3.to_excel(writer, sheet_name='metabolites', index=False)\n",
    "\n",
    "# 关闭 ExcelWriter 对象\n",
    "writer.close()\n",
    "\n",
    "print(\"任务完成。新文件 '14067gem3.xlsx' 已创建,包含更新后的 metabolites 工作簿以及原始文件的其他工作簿。\")"
   ],
   "id": "4cc9ff6f55358bd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d0816d6c1ec1d734",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
