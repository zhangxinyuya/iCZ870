#!/usr/bin/env python3
"""
Complete Core SNPs phylogenetic tree pipeline - Docker Version (V2)
âœ… ä½¿ç”¨Dockerå®¹å™¨æ›¿ä»£Condaç¯å¢ƒ
âœ… å®Œå…¨é€‚é…NCBI Datasetsä¸‹è½½æ ¼å¼ (GCF_xxx.fna)
âœ… è‡ªåŠ¨ä».fnaæ–‡ä»¶headeræå–èŒæ ªåï¼Œä¿ç•™å®Œæ•´ç‰©ç§å
âœ… å®Œæ•´çš„ANIç­›é€‰ + SNPåˆ†æ + å»ºæ ‘æµç¨‹

Author: Adapted for C. glutamicum analysis
Date: 2024-12
Modified: V2 - å¢åŠ å®Œæ•´ç‰©ç§åæ ‡ç­¾
"""

import os
import subprocess
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from Bio import SeqIO
import shutil
import logging
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional


class SNPTreePipelineDocker:
    """å®Œæ•´çš„SNPç³»ç»Ÿå‘è‚²æ ‘æ„å»ºæµç¨‹ - Dockerç‰ˆæœ¬"""

    def __init__(self,
                 genome_dir: str,
                 output_dir: str,
                 metadata_file: Optional[str] = None,
                 threads: int = 8,
                 ani_threshold: float = 99.9,
                 species_name: str = "Corynebacterium glutamicum",
                 docker_fastani: str = "staphb/fastani:latest",
                 docker_snippy: str = "staphb/snippy:latest",
                 docker_iqtree: str = "staphb/iqtree:latest"):
        """
        å‚æ•°:
        genome_dir: åŸºå› ç»„æ–‡ä»¶å¤¹ï¼ˆgenomes/ç›®å½•ï¼ŒåŒ…å«.fnaæ–‡ä»¶ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        metadata_file: å¯é€‰çš„å…ƒæ•°æ®CSVæ–‡ä»¶
        threads: çº¿ç¨‹æ•°
        ani_threshold: ANIç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ>=æ­¤å€¼è§†ä¸ºå†—ä½™ï¼‰
        species_name: é»˜è®¤ç‰©ç§åï¼ˆç”¨äºæ ‡ç­¾ï¼‰
        docker_fastani: FastANI Dockeré•œåƒ
        docker_snippy: Snippy Dockeré•œåƒ
        docker_iqtree: IQ-TREE Dockeré•œåƒ
        """
        self.genome_dir = Path(genome_dir).resolve()
        self.output_dir = Path(output_dir).resolve()
        self.metadata_file = metadata_file
        self.threads = threads
        self.ani_threshold = ani_threshold
        self.species_name = species_name

        self.docker_fastani = docker_fastani
        self.docker_snippy = docker_snippy
        self.docker_iqtree = docker_iqtree

        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.filtered_genomes_dir = self.output_dir / "filtered_genomes"
        self.filtered_genomes_dir.mkdir(exist_ok=True)
        self.ani_results_dir = self.output_dir / "ani_analysis"
        self.ani_results_dir.mkdir(exist_ok=True)

        self.strain_to_full_label: Dict[str, str] = {}

        self._setup_logging()
        self._verify_docker()
        self.metadata_df = self._load_metadata()

    def _verify_docker(self):
        """éªŒè¯Dockeræ˜¯å¦å¯ç”¨"""
        self.logger.info("\néªŒè¯Dockerç¯å¢ƒ...")

        try:
            result = subprocess.run(["docker", "--version"],
                                    capture_output=True, text=True)
            if result.returncode != 0:
                raise RuntimeError("Dockeræœªå®‰è£…æˆ–æ— æ³•è¿è¡Œ")

            self.logger.info(f"  âœ… {result.stdout.strip()}")

            images_to_check = [
                (self.docker_fastani, "FastANI"),
                (self.docker_snippy, "Snippy"),
                (self.docker_iqtree, "IQ-TREE"),
            ]

            for image, name in images_to_check:
                result = subprocess.run(
                    ["docker", "image", "inspect", image],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    self.logger.info(f"  âœ… {name} é•œåƒå·²å­˜åœ¨: {image}")
                else:
                    self.logger.warning(f"  âš ï¸  {name} é•œåƒæœªæ‰¾åˆ°: {image}")
                    self.logger.warning(f"     å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ä¸‹è½½")

        except Exception as e:
            self.logger.error(f"âŒ DockeréªŒè¯å¤±è´¥: {e}")
            self.logger.error("\nè¯·ç¡®ä¿:")
            self.logger.error("  1. Dockerå·²å®‰è£…")
            self.logger.error("  2. DockeræœåŠ¡æ­£åœ¨è¿è¡Œ")
            self.logger.error("  3. å½“å‰ç”¨æˆ·æœ‰æƒé™è¿è¡ŒDocker")
            sys.exit(1)

    def _get_docker_mount_path(self, local_path: Path) -> Tuple[str, str]:
        """è·å–DockeræŒ‚è½½è·¯å¾„"""
        local_abs = local_path.resolve()
        return str(local_abs), str(local_abs)

    def _run_docker(self, image: str, cmd: str, description: str,
                    mount_dirs: Optional[List[Path]] = None) -> Optional[str]:
        """åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œå‘½ä»¤"""
        self.logger.info(f"\n{'=' * 70}")
        self.logger.info(f"Step: {description}")
        self.logger.info(f"Docker Image: {image}")
        self.logger.info(f"{'=' * 70}\n")

        mount_args = []
        if mount_dirs:
            for dir_path in mount_dirs:
                local_path, container_path = self._get_docker_mount_path(dir_path)
                mount_args.extend(["-v", f"{local_path}:{container_path}"])

        output_local, output_container = self._get_docker_mount_path(self.output_dir)
        mount_args.extend(["-v", f"{output_local}:{output_container}"])

        genome_local, genome_container = self._get_docker_mount_path(self.genome_dir)
        mount_args.extend(["-v", f"{genome_local}:{genome_container}"])

        docker_cmd = [
                         "docker", "run", "--rm",
                         "-u", f"{os.getuid()}:{os.getgid()}",
                     ] + mount_args + [image, "bash", "-c", cmd]

        self.logger.debug(f"Docker command: {' '.join(docker_cmd)}")

        result = subprocess.run(docker_cmd, capture_output=True, text=True)

        if result.returncode != 0:
            self.logger.error(f"âŒ ERROR: {description} failed!")
            self.logger.error(f"STDERR: {result.stderr}")
            self.logger.error(f"STDOUT: {result.stdout}")
            return None

        self.logger.info(f"âœ… {description} completed")
        if result.stdout:
            self.logger.debug(f"Output: {result.stdout[:500]}")

        return result.stdout

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = self.output_dir / "logs"
        log_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"pipeline_{timestamp}.log"

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")

    def _load_metadata(self) -> Optional[pd.DataFrame]:
        """åŠ è½½å…ƒæ•°æ®æ–‡ä»¶"""
        if not self.metadata_file:
            possible_paths = [
                self.genome_dir.parent / "metadata" / "genomes_summary.csv",
                Path("cglutamicum_pangenome/metadata/genomes_summary.csv"),
                self.output_dir / "genomes_summary.csv"
            ]

            for path in possible_paths:
                if path.exists():
                    self.metadata_file = str(path)
                    break

        if self.metadata_file and Path(self.metadata_file).exists():
            self.logger.info(f"åŠ è½½å…ƒæ•°æ®: {self.metadata_file}")
            df = pd.read_csv(self.metadata_file)
            self.logger.info(f"  æ‰¾åˆ° {len(df)} ä¸ªåŸºå› ç»„çš„å…ƒæ•°æ®")
            return df
        else:
            self.logger.warning("æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ï¼Œå°†ä».fna headerè§£æèŒæ ªå")
            return None

    def parse_fna_for_species_and_strain(self, fna_file: Path) -> Tuple[str, str]:
        """
        ä».fnaæ–‡ä»¶çš„ç¬¬ä¸€è¡Œheaderè§£æç‰©ç§åå’ŒèŒæ ªå
        è¿”å›: (ç‰©ç§å, èŒæ ªå·)
        """
        try:
            with open(fna_file, 'r') as f:
                first_line = f.readline().strip()

            if not first_line.startswith('>'):
                return self.species_name, fna_file.stem

            header = first_line[1:]
            parts = header.split(' ', 1)
            if len(parts) < 2:
                return self.species_name, fna_file.stem

            desc = parts[1]
            words = desc.split()
            species = self.species_name
            strain = ""

            if len(words) >= 2 and words[0][0].isupper():
                if words[1][0].islower():
                    species = f"{words[0]} {words[1]}"
                    remaining = ' '.join(words[2:])
                    remaining = re.sub(r',?\s*(complete\s+)?(sequence|genome|chromosome).*$',
                                       '', remaining, flags=re.IGNORECASE)
                    remaining = re.sub(r'\s*,\s*$', '', remaining)

                    strain_match = re.search(r'(ATCC\s*\d+|strain\s+\S+|str\.\s*\S+|\S+)',
                                             remaining, flags=re.IGNORECASE)
                    if strain_match:
                        strain = strain_match.group(1).strip()
                        strain = re.sub(r'ATCC\s*', 'ATCC ', strain, flags=re.IGNORECASE)

            if not strain:
                filename = fna_file.stem
                atcc_match = re.search(r'(ATCC[_\s]*\d+)', filename, flags=re.IGNORECASE)
                if atcc_match:
                    strain = re.sub(r'ATCC[_\s]*', 'ATCC ', atcc_match.group(1))
                else:
                    strain = filename

            return species, strain.strip()

        except Exception as e:
            self.logger.warning(f"è§£æå¤±è´¥ {fna_file.name}: {e}")
            return self.species_name, fna_file.stem

    def parse_fna_for_species_name(self, fna_file: Path) -> str:
        """ä».fnaæ–‡ä»¶çš„ç¬¬ä¸€è¡Œheaderè§£æèŒæ ªåï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        species, strain = self.parse_fna_for_species_and_strain(fna_file)
        return strain if strain else fna_file.stem

    def run_command(self, cmd: str, description: str) -> Optional[str]:
        """è¿è¡Œæœ¬åœ°å‘½ä»¤ï¼ˆä¸ä½¿ç”¨Dockerï¼‰"""
        self.logger.info(f"\n{'=' * 70}")
        self.logger.info(f"Step: {description}")
        self.logger.debug(f"Command: {cmd}")
        self.logger.info(f"{'=' * 70}\n")

        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

        if result.returncode != 0:
            self.logger.error(f"âŒ ERROR: {description} failed!")
            self.logger.error(f"STDERR: {result.stderr}")
            return None

        self.logger.info(f"âœ… {description} completed")
        return result.stdout

    def step0_prepare_genomes(self) -> List[Path]:
        """Step 0: å‡†å¤‡å’Œæ ‡å‡†åŒ–åŸºå› ç»„æ–‡ä»¶"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("STEP 0: Preparing and standardizing genome files")
        self.logger.info("=" * 70)

        genome_files = list(self.genome_dir.glob("*.fna")) + \
                       list(self.genome_dir.glob("*.fasta")) + \
                       list(self.genome_dir.glob("*.fa"))

        if not genome_files:
            self.logger.error(f"âŒ æœªæ‰¾åˆ°åŸºå› ç»„æ–‡ä»¶åœ¨ {self.genome_dir}")
            sys.exit(1)

        self.logger.info(f"æ‰¾åˆ° {len(genome_files)} ä¸ªåŸºå› ç»„æ–‡ä»¶")

        std_genome_dir = self.output_dir / "standardized_genomes"
        std_genome_dir.mkdir(exist_ok=True)

        gcf_to_strain = {}
        genome_info = []

        for genome_file in genome_files:
            gcf_id = genome_file.stem.replace('_genomic', '')
            species_name, strain_name = self.parse_fna_for_species_and_strain(genome_file)

            if self.metadata_df is not None:
                match = self.metadata_df[self.metadata_df['accession'] == gcf_id]
                if not match.empty:
                    if 'strain' in match.columns:
                        metadata_strain = str(match['strain'].iloc[0])
                        if pd.notna(metadata_strain) and metadata_strain != 'N/A':
                            strain_name = metadata_strain
                    if 'organism_name' in match.columns:
                        org_name = str(match['organism_name'].iloc[0])
                        if pd.notna(org_name) and org_name != 'N/A':
                            words = org_name.split()
                            if len(words) >= 2:
                                species_name = f"{words[0]} {words[1]}"

            clean_strain = self._clean_strain_name(strain_name)

            original_strain = clean_strain
            counter = 1
            while clean_strain in gcf_to_strain.values():
                clean_strain = f"{original_strain}_{counter}"
                counter += 1

            gcf_to_strain[gcf_id] = clean_strain
            full_label = f"{species_name} {strain_name}"
            self.strain_to_full_label[clean_strain] = full_label

            records = list(SeqIO.parse(genome_file, "fasta"))
            total_length = sum(len(rec.seq) for rec in records)
            n_contigs = len(records)

            genome_info.append({
                'accession': gcf_id,
                'species_name': species_name,
                'strain_name': strain_name,
                'clean_strain_name': clean_strain,
                'full_label': full_label,
                'n_contigs': n_contigs,
                'total_length_bp': total_length,
                'size_mbp': round(total_length / 1e6, 2),
                'original_file': genome_file.name
            })

            std_file = std_genome_dir / f"{clean_strain}.fasta"
            shutil.copy2(genome_file, std_file)

            self.logger.info(f"  {gcf_id} â†’ {clean_strain}")
            self.logger.info(f"      å®Œæ•´æ ‡ç­¾: {full_label} ({n_contigs} contigs, {total_length / 1e6:.2f} Mbp)")

        mapping_df = pd.DataFrame(genome_info)
        mapping_file = self.output_dir / "accession_to_strain_mapping.csv"
        mapping_df.to_csv(mapping_file, index=False)

        label_mapping_file = self.output_dir / "strain_label_mapping.csv"
        label_df = pd.DataFrame([
            {'strain': k, 'full_label': v}
            for k, v in self.strain_to_full_label.items()
        ])
        label_df.to_csv(label_mapping_file, index=False)

        self.logger.info(f"\nâœ… æ˜ å°„æ–‡ä»¶å·²ä¿å­˜: {mapping_file}")
        self.logger.info(f"âœ… æ ‡ç­¾æ˜ å°„æ–‡ä»¶: {label_mapping_file}")
        self.logger.info(f"âœ… æ ‡å‡†åŒ–åŸºå› ç»„ç›®å½•: {std_genome_dir}")

        self.genome_dir = std_genome_dir

        return list(std_genome_dir.glob("*.fasta"))

    def _clean_strain_name(self, name: str) -> str:
        """æ¸…ç†èŒæ ªåç§°"""
        name = re.sub(r'[^\w\s\-\.]', '_', name)
        name = name.replace(' ', '_')
        name = re.sub(r'_+', '_', name)
        name = name.strip('_')
        if len(name) > 50:
            name = name[:50]
        return name if name else "Unknown"

    def step1_calculate_ani_fastani(self, genomes: List[Path]) -> Path:
        """Step 1: ä½¿ç”¨FastANIè®¡ç®—ANI"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("STEP 1: Calculating ANI with FastANI (Docker)")
        self.logger.info("=" * 70)

        genome_list = self.ani_results_dir / "genome_list.txt"
        with open(genome_list, 'w') as f:
            for genome in genomes:
                f.write(f"{genome.resolve()}\n")

        ani_output = self.ani_results_dir / "fastani_results.txt"
        cmd = f"fastANI --ql {genome_list} --rl {genome_list} -o {ani_output} -t {self.threads}"

        result = self._run_docker(
            self.docker_fastani, cmd,
            "FastANI calculation",
            mount_dirs=[self.ani_results_dir]
        )

        if result is None or not ani_output.exists():
            self.logger.error("âŒ FastANI failed!")
            sys.exit(1)

        return ani_output

    def step2_parse_ani_results(self, ani_file: Path) -> pd.DataFrame:
        """Step 2: è§£æANIç»“æœä¸ºçŸ©é˜µ"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("STEP 2: Parsing ANI results")
        self.logger.info("=" * 70)

        ani_data = []
        with open(ani_file, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 3:
                    query = Path(parts[0]).stem
                    ref = Path(parts[1]).stem
                    ani = float(parts[2])
                    ani_data.append({'query': query, 'reference': ref, 'ANI': ani})

        ani_df = pd.DataFrame(ani_data)
        strains = sorted(set(ani_df['query'].unique()) | set(ani_df['reference'].unique()))
        ani_matrix = pd.DataFrame(99.9, index=strains, columns=strains)

        for _, row in ani_df.iterrows():
            ani_matrix.loc[row['query'], row['reference']] = row['ANI']
            ani_matrix.loc[row['reference'], row['query']] = row['ANI']

        ani_matrix.to_csv(self.ani_results_dir / 'ani_matrix.csv')

        self.logger.info(f"ANIçŸ©é˜µ: {ani_matrix.shape[0]} x {ani_matrix.shape[1]}")
        self.logger.info(f"ANIèŒƒå›´: {ani_matrix.values.min():.2f}% - {ani_matrix.values.max():.2f}%")

        return ani_matrix

    def step3_filter_redundant_genomes(self, ani_matrix: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """Step 3: ANIç­›é€‰å»å†—ä½™"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info(f"STEP 3: Filtering redundant genomes (ANI >= {self.ani_threshold}%)")
        self.logger.info("=" * 70)

        strains = list(ani_matrix.index)
        n_total = len(strains)

        redundant_pairs = []
        for i in range(len(strains)):
            for j in range(i + 1, len(strains)):
                ani = ani_matrix.iloc[i, j]
                if ani >= self.ani_threshold:
                    redundant_pairs.append((strains[i], strains[j], ani))

        self.logger.info(f"å‘ç° {len(redundant_pairs)} å¯¹å†—ä½™åŸºå› ç»„ (ANI >= {self.ani_threshold}%)")

        if redundant_pairs:
            redundant_df = pd.DataFrame(redundant_pairs, columns=['Strain1', 'Strain2', 'ANI'])
            redundant_df.to_csv(self.ani_results_dir / 'redundant_pairs.csv', index=False)

        clusters = self._cluster_similar_genomes(strains, redundant_pairs)

        selected_strains = []
        removed_strains = []

        for cluster in clusters:
            rep = self._select_representative(cluster)
            selected_strains.append(rep)
            for strain in cluster:
                if strain != rep:
                    removed_strains.append(strain)

        self.logger.info(f"\nğŸ“Š ç­›é€‰ç»“æœ:")
        self.logger.info(f"  åŸå§‹åŸºå› ç»„æ•°: {n_total}")
        self.logger.info(f"  ä¿ç•™åŸºå› ç»„æ•°: {len(selected_strains)}")
        self.logger.info(f"  ç§»é™¤åŸºå› ç»„æ•°: {len(removed_strains)}")

        pd.DataFrame({'strain': selected_strains}).to_csv(
            self.output_dir / 'selected_strains.csv', index=False)
        pd.DataFrame({'strain': removed_strains}).to_csv(
            self.output_dir / 'removed_strains.csv', index=False)

        return selected_strains, removed_strains

    def _cluster_similar_genomes(self, strains: List[str],
                                 redundant_pairs: List[Tuple]) -> List[List[str]]:
        """å¹¶æŸ¥é›†èšç±»"""
        parent = {s: s for s in strains}

        def find(x):
            if parent[x] != x:
                parent[x] = find(parent[x])
            return parent[x]

        def union(x, y):
            rx, ry = find(x), find(y)
            if rx != ry:
                parent[rx] = ry

        for s1, s2, _ in redundant_pairs:
            union(s1, s2)

        clusters = {}
        for s in strains:
            root = find(s)
            clusters.setdefault(root, []).append(s)

        return list(clusters.values())

    def _select_representative(self, cluster: List[str]) -> str:
        """é€‰æ‹©èšç±»ä»£è¡¨"""
        if len(cluster) == 1:
            return cluster[0]

        refs = ['SCgG2', 'ATCC_13032', 'ATCC_14067', 'ATCC_21799']
        for ref in refs:
            for strain in cluster:
                if ref in strain:
                    return strain

        return min(cluster, key=len)

    def step4_copy_selected_genomes(self, selected_strains: List[str]):
        """Step 4: å¤åˆ¶ç­›é€‰åçš„åŸºå› ç»„"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("STEP 4: Copying selected genomes")
        self.logger.info("=" * 70)

        copied = 0
        for strain in selected_strains:
            src = self.genome_dir / f"{strain}.fasta"
            dst = self.filtered_genomes_dir / f"{strain}.fasta"

            if src.exists():
                shutil.copy2(src, dst)
                copied += 1
            else:
                self.logger.warning(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {strain}")

        self.logger.info(f"âœ… å¤åˆ¶äº† {copied}/{len(selected_strains)} ä¸ªåŸºå› ç»„")

    def step5_select_reference_genome(self, selected_strains: List[str]) -> Path:
        """Step 5: é€‰æ‹©å‚è€ƒåŸºå› ç»„"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("STEP 5: Selecting reference genome")
        self.logger.info("=" * 70)

        preferred = ['SCgG2', 'ATCC_13032', 'ATCC_14067']

        ref_strain = None
        for pref in preferred:
            for strain in selected_strains:
                if pref in strain:
                    ref_strain = strain
                    break
            if ref_strain:
                break

        if not ref_strain:
            ref_strain = selected_strains[0]

        ref_path = self.filtered_genomes_dir / f"{ref_strain}.fasta"

        if not ref_path.exists():
            self.logger.error(f"âŒ å‚è€ƒåŸºå› ç»„ä¸å­˜åœ¨: {ref_path}")
            sys.exit(1)

        self.logger.info(f"âœ… é€‰æ‹©å‚è€ƒåŸºå› ç»„: {ref_strain}")

        return ref_path

    def step6_snippy_calling(self, ref_genome: Path):
        """Step 6: Snippy SNP calling"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("STEP 6: SNP calling with Snippy (Docker)")
        self.logger.info("=" * 70)

        snippy_dir = self.output_dir / "snippy_results"
        snippy_dir.mkdir(exist_ok=True)

        genomes = list(self.filtered_genomes_dir.glob("*.fasta"))
        ref_stem = ref_genome.stem

        self.logger.info(f"å¯¹ {len(genomes) - 1} ä¸ªèŒæ ªè¿è¡ŒSnippy")

        for genome in genomes:
            strain = genome.stem

            if strain == ref_stem:
                self.logger.info(f"  âŠ˜ è·³è¿‡å‚è€ƒ: {strain}")
                continue

            outdir = snippy_dir / strain

            if outdir.exists():
                shutil.rmtree(outdir)

            self.logger.info(f"  â†’ å¤„ç†: {strain}")

            cmd = f"snippy --force --outdir {outdir} --ref {ref_genome} --ctgs {genome} --cpus {self.threads}"
            result = self._run_docker(
                self.docker_snippy, cmd,
                f"Snippy-{strain}",
                mount_dirs=[snippy_dir, self.filtered_genomes_dir]
            )

            if result is None:
                self.logger.warning(f"  âš ï¸  Snippyå¤„ç†å¤±è´¥ {strain}")

    def step7_core_snps(self, ref_genome: Path):
        """Step 7: æå–Core SNPs"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("STEP 7: Extracting core SNPs (Docker)")
        self.logger.info("=" * 70)

        snippy_results_dir = self.output_dir / "snippy_results"

        snippy_dirs = [d for d in snippy_results_dir.glob("*")
                       if d.is_dir() and (d / "snps.vcf").exists()]

        if not snippy_dirs:
            self.logger.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„Snippyç»“æœ!")
            sys.exit(1)

        self.logger.info(f"æ‰¾åˆ° {len(snippy_dirs)} ä¸ªæœ‰æ•ˆçš„Snippyç»“æœç›®å½•")

        snippy_dirs_str = " ".join(str(d) for d in snippy_dirs)

        cmd = f"cd {self.output_dir} && snippy-core --ref {ref_genome} --prefix core {snippy_dirs_str}"
        result = self._run_docker(
            self.docker_snippy, cmd,
            "Snippy-core",
            mount_dirs=[snippy_results_dir]
        )

        if result is None:
            self.logger.error("âŒ snippy-core å¤±è´¥!")
            return

        core_aln = self.output_dir / "core.aln"
        core_snps_aln = self.output_dir / "core.snps.aln"

        if not core_aln.exists():
            self.logger.error(f"âŒ core.aln ä¸å­˜åœ¨")
            return

        cmd = f"snp-sites -c {core_aln} > {core_snps_aln}"
        self._run_docker(self.docker_snippy, cmd, "Extract SNP sites", mount_dirs=[])

        if core_snps_aln.exists():
            cmd = f"snp-sites -C {core_aln}"
            result = self._run_docker(self.docker_snippy, cmd, "SNP statistics", mount_dirs=[])
            if result:
                self.logger.info(f"\n{result}")

    def step8_build_tree(self, method: str = 'iqtree') -> Optional[Path]:
        """Step 8: æ„å»ºç³»ç»Ÿå‘è‚²æ ‘"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info(f"STEP 8: Building phylogenetic tree ({method}) (Docker)")
        self.logger.info("=" * 70)

        snp_aln = self.output_dir / "core.snps.aln"

        if not snp_aln.exists():
            self.logger.error(f"âŒ SNPæ¯”å¯¹æ–‡ä»¶ä¸å­˜åœ¨: {snp_aln}")
            return None

        if method == 'iqtree':
            prefix = self.output_dir / "core_snps"
            cmd = f"iqtree -s {snp_aln} -m MFP -bb 1000 -nt {self.threads} -pre {prefix} -redo"

            self._run_docker(self.docker_iqtree, cmd, "IQ-TREE", mount_dirs=[])

            tree_file = self.output_dir / "core_snps.treefile"
        else:
            self.logger.error(f"æœªçŸ¥çš„å»ºæ ‘æ–¹æ³•: {method}")
            return None

        if tree_file.exists():
            self.logger.info(f"âœ… ç³»ç»Ÿå‘è‚²æ ‘å·²ç”Ÿæˆ: {tree_file}")
            self._create_labeled_tree(tree_file)
            return tree_file
        else:
            self.logger.error(f"âŒ æ ‘æ–‡ä»¶åˆ›å»ºå¤±è´¥")
            return None

    def _create_labeled_tree(self, tree_file: Path):
        """åˆ›å»ºå¸¦æœ‰å®Œæ•´ç‰©ç§åæ ‡ç­¾çš„æ ‘æ–‡ä»¶"""
        self.logger.info("\nåˆ›å»ºå¸¦å®Œæ•´æ ‡ç­¾çš„æ ‘æ–‡ä»¶...")

        with open(tree_file, 'r') as f:
            tree_content = f.read()

        # è°ƒè¯•ï¼šæ‰“å°æ ‘æ–‡ä»¶ä¸­çš„å‰å‡ ä¸ªæ ‡ç­¾å’Œæ˜ å°„
        self.logger.info(f"  æ˜ å°„è¡¨ä¸­æœ‰ {len(self.strain_to_full_label)} ä¸ªæ¡ç›®")
        if self.strain_to_full_label:
            sample_items = list(self.strain_to_full_label.items())[:3]
            for strain, label in sample_items:
                self.logger.info(f"    æ˜ å°„ç¤ºä¾‹: '{strain}' -> '{label}'")

        # æå–æ ‘æ–‡ä»¶ä¸­çš„æ ‡ç­¾ï¼ˆåœ¨newickæ ¼å¼ä¸­ï¼Œæ ‡ç­¾åœ¨æ‹¬å·å’Œå†’å·ä¹‹é—´ï¼‰
        tree_labels = re.findall(r'[(),]([A-Za-z0-9_.\-]+):', tree_content)
        if tree_labels:
            self.logger.info(f"  æ ‘æ–‡ä»¶ä¸­æ‰¾åˆ° {len(tree_labels)} ä¸ªæ ‡ç­¾")
            self.logger.info(f"    æ ‘æ ‡ç­¾ç¤ºä¾‹: {tree_labels[:3]}")

        labeled_tree = tree_content
        replaced_count = 0

        # æŒ‰åç§°é•¿åº¦é™åºæ’åˆ—ï¼Œé¿å…çŸ­åç§°å…ˆè¢«æ›¿æ¢å¯¼è‡´é•¿åç§°æ— æ³•åŒ¹é…
        sorted_strains = sorted(self.strain_to_full_label.items(),
                                key=lambda x: len(x[0]), reverse=True)

        for strain, full_label in sorted_strains:
            # å°†ç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼ˆnewickæ ¼å¼ä¸æ”¯æŒç©ºæ ¼ï¼‰
            safe_label = full_label.replace(' ', '_')
            # åœ¨newickæ ¼å¼ä¸­ï¼Œæ ‡ç­¾åé¢é€šå¸¸æ˜¯ : ) , æˆ–æ¢è¡Œ
            pattern = rf'(?<![A-Za-z0-9_]){re.escape(strain)}(?![A-Za-z0-9_])'

            # æ£€æŸ¥æ˜¯å¦èƒ½åŒ¹é…åˆ°
            if re.search(pattern, labeled_tree):
                labeled_tree = re.sub(pattern, safe_label, labeled_tree)
                replaced_count += 1

        self.logger.info(f"  æˆåŠŸæ›¿æ¢äº† {replaced_count}/{len(self.strain_to_full_label)} ä¸ªæ ‡ç­¾")

        labeled_tree_file = self.output_dir / "core_snps.labeled.treefile"
        with open(labeled_tree_file, 'w') as f:
            f.write(labeled_tree)

        self.logger.info(f"âœ… å¸¦æ ‡ç­¾çš„æ ‘æ–‡ä»¶: {labeled_tree_file}")

    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("ğŸ§¬ Core SNPs Phylogenetic Tree Pipeline (Docker Version V2)")
        self.logger.info("=" * 70)
        self.logger.info(f"è¾“å…¥ç›®å½•: {self.genome_dir}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info(f"ANIé˜ˆå€¼: {self.ani_threshold}%")
        self.logger.info(f"é»˜è®¤ç‰©ç§å: {self.species_name}")

        try:
            genomes = self.step0_prepare_genomes()
            ani_file = self.step1_calculate_ani_fastani(genomes)
            ani_matrix = self.step2_parse_ani_results(ani_file)
            selected, removed = self.step3_filter_redundant_genomes(ani_matrix)
            self.step4_copy_selected_genomes(selected)
            ref_genome = self.step5_select_reference_genome(selected)
            self.step6_snippy_calling(ref_genome)
            self.step7_core_snps(ref_genome)
            tree_file = self.step8_build_tree(method='iqtree')

            self._print_final_summary(len(genomes), len(selected), len(removed))

        except KeyboardInterrupt:
            self.logger.warning("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æµç¨‹")
            sys.exit(1)
        except Exception as e:
            self.logger.error(f"\n\nâŒ æµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    def _print_final_summary(self, total: int, selected: int, removed: int):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("ğŸ‰ æµç¨‹å®Œæˆ!")
        self.logger.info("=" * 70)
        self.logger.info(f"åŸå§‹åŸºå› ç»„: {total}")
        self.logger.info(f"ç­›é€‰å: {selected} (ç§»é™¤{removed}ä¸ª)")
        self.logger.info(f"\nä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        self.logger.info(f"  1. ANIçŸ©é˜µ: {self.ani_results_dir}/ani_matrix.csv")
        self.logger.info(f"  2. ç­›é€‰ç»“æœ: {self.output_dir}/selected_strains.csv")
        self.logger.info(f"  3. æ ‡ç­¾æ˜ å°„: {self.output_dir}/strain_label_mapping.csv")
        self.logger.info(f"  4. ç³»ç»Ÿå‘è‚²æ ‘: {self.output_dir}/core_snps.treefile")
        self.logger.info(f"  5. å¸¦æ ‡ç­¾çš„æ ‘: {self.output_dir}/core_snps.labeled.treefile")
        self.logger.info("=" * 70)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Complete Core SNPs phylogenetic tree pipeline (Docker Version V2)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ç”¨æ³•
  python run_complete_pipeline_docker_v2.py -i genomes -o results

  # æŒ‡å®šç‰©ç§å
  python run_complete_pipeline_docker_v2.py -i genomes -o results \\
      --species "Corynebacterium glutamicum"

  # æŒ‡å®šçº¿ç¨‹æ•°å’ŒANIé˜ˆå€¼
  python run_complete_pipeline_docker_v2.py -i genomes -o results -t 16 --ani-threshold 99.9

ç¯å¢ƒå‡†å¤‡:
  # æ‹‰å–Dockeré•œåƒ
  docker pull staphb/fastani:latest
  docker pull staphb/snippy:latest
  docker pull staphb/iqtree:latest
        """
    )

    parser.add_argument('-i', '--input', required=True,
                        help='è¾“å…¥ç›®å½•ï¼ˆåŒ…å«.fnaåŸºå› ç»„æ–‡ä»¶ï¼‰')
    parser.add_argument('-o', '--output', default='snp_tree_results',
                        help='è¾“å‡ºç›®å½• (default: snp_tree_results)')
    parser.add_argument('-t', '--threads', type=int, default=8,
                        help='çº¿ç¨‹æ•° (default: 8)')
    parser.add_argument('--ani-threshold', type=float, default=99.9,
                        help='ANIé˜ˆå€¼ (default: 99.9)')
    parser.add_argument('--metadata', type=str, default=None,
                        help='å…ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--species', type=str,
                        default='Corynebacterium glutamicum',
                        help='é»˜è®¤ç‰©ç§å (default: Corynebacterium glutamicum)')

    # Dockeré•œåƒé…ç½®
    parser.add_argument('--docker-fastani', type=str,
                        default='staphb/fastani:latest',
                        help='FastANI Dockeré•œåƒ')
    parser.add_argument('--docker-snippy', type=str,
                        default='staphb/snippy:latest',
                        help='Snippy Dockeré•œåƒ')
    parser.add_argument('--docker-iqtree', type=str,
                        default='staphb/iqtree:latest',
                        help='IQ-TREE Dockeré•œåƒ')

    args = parser.parse_args()

    pipeline = SNPTreePipelineDocker(
        genome_dir=args.input,
        output_dir=args.output,
        metadata_file=args.metadata,
        threads=args.threads,
        ani_threshold=args.ani_threshold,
        species_name=args.species,
        docker_fastani=args.docker_fastani,
        docker_snippy=args.docker_snippy,
        docker_iqtree=args.docker_iqtree
    )

    pipeline.run_full_pipeline()


if __name__ == "__main__":
    main()