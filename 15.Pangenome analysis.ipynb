{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ä»NCBIä¸‹è½½æ‰€æœ‰C. glutamicumåŸºå› ç»„",
   "id": "d4d89ba98d55954c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T02:19:20.219369Z",
     "start_time": "2025-12-19T02:15:26.382903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "NCBI Datasets åˆ†æ‰¹ä¸‹è½½è„šæœ¬(ä¼˜åŒ–ç‰ˆ)\n",
    "ç”¨äºä¸‹è½½ Corynebacterium glutamicum çš„åŸºå› ç»„æ•°æ®\n",
    "ç‰¹æ€§:\n",
    "- è¿›åº¦æ¡æ˜¾ç¤º\n",
    "- å•ä¸ªåŸºå› ç»„çº§åˆ«çš„é‡è¯•\n",
    "- æ–­ç‚¹ç»­ä¼ \n",
    "- è¯¦ç»†æ—¥å¿—è®°å½•\n",
    "- åªä¸‹è½½ RefSeq (GCF) åŸºå› ç»„\n",
    "- åŒ…å« CDS åºåˆ—\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"è­¦å‘Š: æœªå®‰è£… tqdm åº“,å°†ä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º\")\n",
    "    print(\"å»ºè®®å®‰è£…: pip install tqdm\")\n",
    "    tqdm = None\n",
    "\n",
    "\n",
    "class DownloadLogger:\n",
    "    \"\"\"æ—¥å¿—ç®¡ç†å™¨\"\"\"\n",
    "    def __init__(self, log_dir: str = \"logs\"):\n",
    "        Path(log_dir).mkdir(exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = os.path.join(log_dir, f\"download_{timestamp}.log\")\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file, encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(f\"æ—¥å¿—æ–‡ä»¶: {log_file}\")\n",
    "\n",
    "\n",
    "def get_genome_accessions(taxon_name: str,\n",
    "                          assembly_source: str = \"refseq\",\n",
    "                          assembly_level: str = \"complete\") -> List[str]:\n",
    "    \"\"\"è·å–åŸºå› ç»„åˆ—è¡¨\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"æ­£åœ¨è·å– {taxon_name} çš„åŸºå› ç»„åˆ—è¡¨...\")\n",
    "    logger.info(f\"æ•°æ®æ¥æº: {assembly_source}\")\n",
    "    logger.info(f\"ç»„è£…çº§åˆ«: {assembly_level}\")\n",
    "\n",
    "    cmd = [\n",
    "        \"datasets\", \"summary\", \"genome\", \"taxon\", taxon_name,\n",
    "        \"--assembly-source\", assembly_source,\n",
    "        \"--assembly-level\", assembly_level,\n",
    "        \"--as-json-lines\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        accessions = []\n",
    "\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            if line:\n",
    "                data = json.loads(line)\n",
    "                accession = data.get('accession')\n",
    "                if accession:\n",
    "                    accessions.append(accession)\n",
    "\n",
    "        logger.info(f\"æ‰¾åˆ° {len(accessions)} ä¸ª {assembly_source.upper()} åŸºå› ç»„\")\n",
    "        return accessions\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"æ— æ³•è·å–åŸºå› ç»„åˆ—è¡¨: {e.stderr}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def check_existing_downloads(output_dir: str) -> set:\n",
    "    \"\"\"æ£€æŸ¥å·²ä¸‹è½½çš„æ‰¹æ¬¡\"\"\"\n",
    "    existing = set()\n",
    "    if not os.path.exists(output_dir):\n",
    "        return existing\n",
    "\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.startswith(\"cglutamicum_batch_\") and filename.endswith(\".zip\"):\n",
    "            # æå–æ‰¹æ¬¡å·\n",
    "            batch_num = int(filename.split(\"_\")[2].split(\".\")[0])\n",
    "            # æ£€æŸ¥æ–‡ä»¶å¤§å°(è‡³å°‘1KBæ‰ç®—æœ‰æ•ˆ)\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            if os.path.getsize(filepath) > 1024:\n",
    "                existing.add(batch_num)\n",
    "\n",
    "    return existing\n",
    "\n",
    "\n",
    "def download_single_genome(accession: str, output_dir: str, include_types: str,\n",
    "                          assembly_source: str = \"refseq\",\n",
    "                          max_retries: int = 3) -> bool:\n",
    "    \"\"\"\n",
    "    ä¸‹è½½å•ä¸ªåŸºå› ç»„(å¸¦é‡è¯•)\n",
    "\n",
    "    å‚æ•°:\n",
    "        accession: åŸºå› ç»„ç™»å½•å·\n",
    "        output_dir: è¾“å‡ºç›®å½•\n",
    "        include_types: è¦ä¸‹è½½çš„æ•°æ®ç±»å‹\n",
    "        assembly_source: æ•°æ®æ¥æº\n",
    "        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    filename = f\"{accession}.zip\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°>1KB,è·³è¿‡\n",
    "    if os.path.exists(filepath) and os.path.getsize(filepath) > 1024:\n",
    "        logger.debug(f\"è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {accession}\")\n",
    "        return True\n",
    "\n",
    "    cmd = [\n",
    "        \"datasets\", \"download\", \"genome\", \"accession\",\n",
    "        accession,\n",
    "        \"--include\", include_types,\n",
    "        \"--assembly-source\", assembly_source,  # æŒ‡å®šæ•°æ®æ¥æº\n",
    "        \"--filename\", filepath\n",
    "    ]\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            subprocess.run(cmd, check=True, capture_output=True)\n",
    "            logger.debug(f\"âœ“ {accession} ä¸‹è½½æˆåŠŸ\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"âœ— {accession} ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                logger.error(f\"âœ— {accession} ä¸‹è½½å¤±è´¥ (å·²é‡è¯•{max_retries}æ¬¡)\")\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def download_batch(accessions: List[str], batch_num: int, output_dir: str,\n",
    "                   include_types: str, assembly_source: str = \"refseq\",\n",
    "                   max_retries: int = 3) -> Tuple[int, List[str]]:\n",
    "    \"\"\"\n",
    "    ä¸‹è½½ä¸€æ‰¹åŸºå› ç»„(é€ä¸ªä¸‹è½½ä»¥æ”¯æŒæ–­ç‚¹ç»­ä¼ )\n",
    "    è¿”å›: (æˆåŠŸæ•°é‡, å¤±è´¥çš„accessionsåˆ—è¡¨)\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    batch_dir = os.path.join(output_dir, f\"batch_{batch_num:03d}\")\n",
    "    Path(batch_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"æ‰¹æ¬¡ {batch_num}: {len(accessions)} ä¸ªåŸºå› ç»„\")\n",
    "    logger.info(f\"ä¿å­˜åˆ°: {batch_dir}/\")\n",
    "\n",
    "    successful = 0\n",
    "    failed_accessions = []\n",
    "\n",
    "    # ä½¿ç”¨è¿›åº¦æ¡\n",
    "    iterator = tqdm(accessions, desc=f\"æ‰¹æ¬¡ {batch_num}\",\n",
    "                   unit=\"genome\", ncols=100) if tqdm else accessions\n",
    "\n",
    "    for accession in iterator:\n",
    "        if download_single_genome(accession, batch_dir, include_types,\n",
    "                                  assembly_source, max_retries):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed_accessions.append(accession)\n",
    "\n",
    "        # å¦‚æœä¸ä½¿ç”¨tqdm,æ‰‹åŠ¨æ‰“å°è¿›åº¦\n",
    "        if not tqdm:\n",
    "            print(f\"  è¿›åº¦: {successful}/{len(accessions)} (å¤±è´¥: {len(failed_accessions)})\")\n",
    "\n",
    "    # å¦‚æœå…¨éƒ¨æˆåŠŸ,åˆ›å»ºæ‰¹æ¬¡æ ‡è®°æ–‡ä»¶\n",
    "    if not failed_accessions:\n",
    "        marker_file = os.path.join(output_dir, f\"batch_{batch_num:03d}_complete.marker\")\n",
    "        with open(marker_file, 'w') as f:\n",
    "            f.write(f\"Completed at {datetime.now()}\\n\")\n",
    "            f.write(f\"Total genomes: {len(accessions)}\\n\")\n",
    "\n",
    "    logger.info(f\"æ‰¹æ¬¡ {batch_num} å®Œæˆ: {successful}/{len(accessions)} æˆåŠŸ\")\n",
    "\n",
    "    return successful, failed_accessions\n",
    "\n",
    "\n",
    "def retry_failed_downloads(failed_map: dict, output_dir: str, include_types: str,\n",
    "                          assembly_source: str = \"refseq\") -> dict:\n",
    "    \"\"\"é‡è¯•æ‰€æœ‰å¤±è´¥çš„ä¸‹è½½\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if not failed_map:\n",
    "        return {}\n",
    "\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"å¼€å§‹é‡è¯•å¤±è´¥çš„ä¸‹è½½...\")\n",
    "    logger.info(f\"å¤±è´¥æ‰¹æ¬¡æ•°: {len(failed_map)}\")\n",
    "    total_failed = sum(len(accs) for accs in failed_map.values())\n",
    "    logger.info(f\"å¤±è´¥åŸºå› ç»„æ€»æ•°: {total_failed}\")\n",
    "\n",
    "    still_failed = {}\n",
    "\n",
    "    for batch_num, accessions in failed_map.items():\n",
    "        logger.info(f\"\\né‡è¯•æ‰¹æ¬¡ {batch_num}: {len(accessions)} ä¸ªåŸºå› ç»„\")\n",
    "        batch_dir = os.path.join(output_dir, f\"batch_{batch_num:03d}\")\n",
    "\n",
    "        successful, failed = download_batch(accessions, batch_num, output_dir,\n",
    "                                           include_types, assembly_source,\n",
    "                                           max_retries=5)\n",
    "\n",
    "        if failed:\n",
    "            still_failed[batch_num] = failed\n",
    "\n",
    "    return still_failed\n",
    "\n",
    "\n",
    "def save_failed_list(failed_map: dict, output_dir: str):\n",
    "    \"\"\"ä¿å­˜å¤±è´¥åˆ—è¡¨åˆ°æ–‡ä»¶\"\"\"\n",
    "    if not failed_map:\n",
    "        return\n",
    "\n",
    "    failed_file = os.path.join(output_dir, \"failed_accessions.txt\")\n",
    "    with open(failed_file, 'w') as f:\n",
    "        f.write(f\"ä¸‹è½½å¤±è´¥çš„åŸºå› ç»„åˆ—è¡¨ - {datetime.now()}\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "        for batch_num, accessions in sorted(failed_map.items()):\n",
    "            f.write(f\"æ‰¹æ¬¡ {batch_num}:\\n\")\n",
    "            for acc in accessions:\n",
    "                f.write(f\"  {acc}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    logging.getLogger(__name__).info(f\"å¤±è´¥åˆ—è¡¨å·²ä¿å­˜åˆ°: {failed_file}\")\n",
    "\n",
    "\n",
    "def print_summary(total_genomes: int, total_batches: int,\n",
    "                 successful_batches: int, failed_map: dict,\n",
    "                 assembly_source: str, include_types: str):\n",
    "    \"\"\"æ‰“å°ä¸‹è½½æ‘˜è¦\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    total_failed = sum(len(accs) for accs in failed_map.values())\n",
    "    total_successful = total_genomes - total_failed\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"ä¸‹è½½å®Œæˆ!\")\n",
    "    logger.info(f\"æ•°æ®æ¥æº: {assembly_source.upper()}\")\n",
    "    logger.info(f\"æ•°æ®ç±»å‹: {include_types}\")\n",
    "    logger.info(f\"æ€»åŸºå› ç»„æ•°: {total_genomes}\")\n",
    "    logger.info(f\"æˆåŠŸä¸‹è½½: {total_successful} ({total_successful/total_genomes*100:.1f}%)\")\n",
    "    logger.info(f\"ä¸‹è½½å¤±è´¥: {total_failed} ({total_failed/total_genomes*100:.1f}%)\")\n",
    "    logger.info(f\"æˆåŠŸæ‰¹æ¬¡: {successful_batches}/{total_batches}\")\n",
    "\n",
    "    if failed_map:\n",
    "        logger.info(f\"\\nå¤±è´¥çš„æ‰¹æ¬¡: {', '.join(map(str, sorted(failed_map.keys())))}\")\n",
    "        logger.info(f\"è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹: failed_accessions.txt\")\n",
    "\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ==================== é…ç½®å‚æ•° ====================\n",
    "    TAXON_NAME = \"Corynebacterium glutamicum\"\n",
    "    BATCH_SIZE = 30\n",
    "    OUTPUT_DIR = \"cglutamicum_downloads\"\n",
    "\n",
    "    # ğŸ‘‡ ä¿®æ”¹1: æ·»åŠ  cds åˆ°ä¸‹è½½ç±»å‹\n",
    "    INCLUDE_TYPES = \"genome,protein,gff3,cds\"\n",
    "\n",
    "    # ğŸ‘‡ ä¿®æ”¹2: åªä¸‹è½½ RefSeq (GCF) åŸºå› ç»„\n",
    "    # å¯é€‰å€¼: \"refseq\" (åªä¸‹è½½GCF), \"genbank\" (åªä¸‹è½½GCA), \"all\" (å…¨éƒ¨ä¸‹è½½)\n",
    "    ASSEMBLY_SOURCE = \"refseq\"\n",
    "    ASSEMBLY_LEVEL = \"complete\"\n",
    "    MAX_RETRIES = 3\n",
    "    ENABLE_RESUME = True  # å¯ç”¨æ–­ç‚¹ç»­ä¼ \n",
    "    # =================================================\n",
    "\n",
    "    # åˆå§‹åŒ–æ—¥å¿—\n",
    "    log_manager = DownloadLogger()\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logger.info(f\"å¼€å§‹ä¸‹è½½ {TAXON_NAME}\")\n",
    "    logger.info(f\"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}\")\n",
    "    logger.info(f\"è¾“å‡ºç›®å½•: {OUTPUT_DIR}\")\n",
    "    logger.info(f\"æ•°æ®ç±»å‹: {INCLUDE_TYPES}\")\n",
    "    logger.info(f\"æ•°æ®æ¥æº: {ASSEMBLY_SOURCE} (GCF=RefSeq, GCA=GenBank)\")\n",
    "    logger.info(f\"æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if ENABLE_RESUME else 'ç¦ç”¨'}\")\n",
    "\n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "    # è·å–æ‰€æœ‰åŸºå› ç»„ç™»å½•å·ï¼ˆåªè·å– RefSeqï¼‰\n",
    "    accessions = get_genome_accessions(TAXON_NAME, ASSEMBLY_SOURCE,ASSEMBLY_LEVEL)\n",
    "\n",
    "    if not accessions:\n",
    "        logger.error(\"æœªæ‰¾åˆ°åŸºå› ç»„æ•°æ®,é€€å‡º\")\n",
    "        return\n",
    "\n",
    "    # æ˜¾ç¤ºè·å–åˆ°çš„åŸºå› ç»„å‰ç¼€ç»Ÿè®¡\n",
    "    gcf_count = sum(1 for acc in accessions if acc.startswith(\"GCF\"))\n",
    "    gca_count = sum(1 for acc in accessions if acc.startswith(\"GCA\"))\n",
    "    logger.info(f\"åŸºå› ç»„ç»Ÿè®¡: GCF={gcf_count}, GCA={gca_count}\")\n",
    "\n",
    "    # æ£€æŸ¥å·²å®Œæˆçš„æ‰¹æ¬¡\n",
    "    completed_batches = set()\n",
    "    if ENABLE_RESUME:\n",
    "        for filename in os.listdir(OUTPUT_DIR):\n",
    "            if filename.endswith(\"_complete.marker\"):\n",
    "                batch_num = int(filename.split(\"_\")[1])\n",
    "                completed_batches.add(batch_num)\n",
    "\n",
    "        if completed_batches:\n",
    "            logger.info(f\"å‘ç°å·²å®Œæˆçš„æ‰¹æ¬¡: {sorted(completed_batches)}\")\n",
    "\n",
    "    # è®¡ç®—æ‰¹æ¬¡æ•°\n",
    "    total_batches = (len(accessions) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    logger.info(f\"\\nå°†åˆ† {total_batches} æ‰¹æ¬¡ä¸‹è½½\")\n",
    "\n",
    "    # åˆ†æ‰¹ä¸‹è½½\n",
    "    successful_batches = 0\n",
    "    failed_map = {}  # {batch_num: [failed_accessions]}\n",
    "\n",
    "    # ä¸»è¿›åº¦æ¡\n",
    "    batch_iterator = range(0, len(accessions), BATCH_SIZE)\n",
    "    if tqdm:\n",
    "        batch_iterator = tqdm(list(batch_iterator), desc=\"æ€»è¿›åº¦\", unit=\"æ‰¹æ¬¡\", ncols=100)\n",
    "\n",
    "    for i in batch_iterator:\n",
    "        batch_num = (i // BATCH_SIZE) + 1\n",
    "\n",
    "        # è·³è¿‡å·²å®Œæˆçš„æ‰¹æ¬¡\n",
    "        if batch_num in completed_batches:\n",
    "            logger.info(f\"è·³è¿‡å·²å®Œæˆçš„æ‰¹æ¬¡ {batch_num}\")\n",
    "            successful_batches += 1\n",
    "            continue\n",
    "\n",
    "        batch_accessions = accessions[i:i + BATCH_SIZE]\n",
    "\n",
    "        successful, failed = download_batch(\n",
    "            batch_accessions, batch_num, OUTPUT_DIR,\n",
    "            INCLUDE_TYPES, ASSEMBLY_SOURCE, MAX_RETRIES\n",
    "        )\n",
    "\n",
    "        if not failed:\n",
    "            successful_batches += 1\n",
    "        else:\n",
    "            failed_map[batch_num] = failed\n",
    "\n",
    "        # æ‰¹æ¬¡é—´å»¶è¿Ÿ\n",
    "        if i + BATCH_SIZE < len(accessions):\n",
    "            time.sleep(1)\n",
    "\n",
    "    # é‡è¯•å¤±è´¥çš„ä¸‹è½½\n",
    "    if failed_map:\n",
    "        logger.info(f\"\\nå‘ç° {len(failed_map)} ä¸ªæ‰¹æ¬¡æœ‰å¤±è´¥çš„ä¸‹è½½\")\n",
    "        response = input(\"æ˜¯å¦é‡è¯•å¤±è´¥çš„ä¸‹è½½? (y/n): \").strip().lower()\n",
    "\n",
    "        if response == 'y':\n",
    "            failed_map = retry_failed_downloads(failed_map, OUTPUT_DIR,\n",
    "                                                INCLUDE_TYPES, ASSEMBLY_SOURCE)\n",
    "\n",
    "    # ä¿å­˜å¤±è´¥åˆ—è¡¨\n",
    "    if failed_map:\n",
    "        save_failed_list(failed_map, OUTPUT_DIR)\n",
    "\n",
    "    # æ‰“å°æ‘˜è¦\n",
    "    print_summary(len(accessions), total_batches, successful_batches,\n",
    "                  failed_map, ASSEMBLY_SOURCE, INCLUDE_TYPES)\n",
    "\n",
    "    logger.info(f\"\\næ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_DIR}/\")\n",
    "    logger.info(f\"\\nä¸‹è½½çš„æ–‡ä»¶ç±»å‹è¯´æ˜:\")\n",
    "    logger.info(f\"  - genome: åŸºå› ç»„åºåˆ— (*_genomic.fna)\")\n",
    "    logger.info(f\"  - protein: è›‹ç™½è´¨åºåˆ— (*_protein.faa)\")\n",
    "    logger.info(f\"  - gff3: GFF3æ³¨é‡Šæ–‡ä»¶ (*_genomic.gff)\")\n",
    "    logger.info(f\"  - cds: CDSæ ¸è‹·é…¸åºåˆ— (*_cds_from_genomic.fna)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "26cf66b1a24f19e5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 10:15:26,413 - INFO - æ—¥å¿—æ–‡ä»¶: logs\\download_20251219_101526.log\n",
      "2025-12-19 10:15:26,414 - INFO - å¼€å§‹ä¸‹è½½ Corynebacterium glutamicum\n",
      "2025-12-19 10:15:26,415 - INFO - æ‰¹æ¬¡å¤§å°: 30\n",
      "2025-12-19 10:15:26,415 - INFO - è¾“å‡ºç›®å½•: cglutamicum_downloads\n",
      "2025-12-19 10:15:26,415 - INFO - æ•°æ®ç±»å‹: genome,protein,gff3,cds\n",
      "2025-12-19 10:15:26,416 - INFO - æ•°æ®æ¥æº: refseq (GCF=RefSeq, GCA=GenBank)\n",
      "2025-12-19 10:15:26,417 - INFO - æ–­ç‚¹ç»­ä¼ : å¯ç”¨\n",
      "2025-12-19 10:15:26,417 - INFO - æ­£åœ¨è·å– Corynebacterium glutamicum çš„åŸºå› ç»„åˆ—è¡¨...\n",
      "2025-12-19 10:15:26,418 - INFO - æ•°æ®æ¥æº: refseq\n",
      "2025-12-19 10:15:26,418 - INFO - ç»„è£…çº§åˆ«: complete\n",
      "2025-12-19 10:15:30,720 - INFO - æ‰¾åˆ° 34 ä¸ª REFSEQ åŸºå› ç»„\n",
      "2025-12-19 10:15:30,720 - INFO - åŸºå› ç»„ç»Ÿè®¡: GCF=34, GCA=0\n",
      "2025-12-19 10:15:30,721 - INFO - \n",
      "å°†åˆ† 2 æ‰¹æ¬¡ä¸‹è½½\n",
      "æ€»è¿›åº¦:   0%|                                                               | 0/2 [00:00<?, ?æ‰¹æ¬¡/s]2025-12-19 10:15:30,734 - INFO - \n",
      "============================================================\n",
      "2025-12-19 10:15:30,734 - INFO - æ‰¹æ¬¡ 1: 30 ä¸ªåŸºå› ç»„\n",
      "2025-12-19 10:15:30,735 - INFO - ä¿å­˜åˆ°: cglutamicum_downloads\\batch_001/\n",
      "\n",
      "æ‰¹æ¬¡ 1:   0%|                                                            | 0/30 [00:00<?, ?genome/s]\u001B[A\n",
      "æ‰¹æ¬¡ 1:   3%|â–ˆâ–‹                                                  | 1/30 [00:05<02:39,  5.51s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:   7%|â–ˆâ–ˆâ–ˆâ–                                                | 2/30 [00:10<02:27,  5.26s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                              | 3/30 [00:17<02:41,  5.96s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                             | 4/30 [00:24<02:41,  6.23s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                           | 5/30 [00:29<02:31,  6.06s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 6/30 [00:35<02:20,  5.86s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 7/30 [00:40<02:09,  5.63s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                      | 8/30 [00:46<02:09,  5.88s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 9/30 [00:55<02:20,  6.70s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 10/30 [01:01<02:13,  6.66s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 11/30 [01:07<01:58,  6.24s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 12/30 [01:14<01:58,  6.57s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 13/30 [01:19<01:45,  6.22s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 14/30 [01:30<01:59,  7.45s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 15/30 [01:37<01:48,  7.25s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 16/30 [01:43<01:39,  7.10s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 17/30 [01:49<01:26,  6.66s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 18/30 [01:57<01:25,  7.10s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 19/30 [02:04<01:18,  7.15s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 20/30 [02:09<01:05,  6.55s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 21/30 [02:15<00:56,  6.26s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 22/30 [02:21<00:48,  6.11s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 23/30 [02:27<00:42,  6.09s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 24/30 [02:40<00:48,  8.15s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 25/30 [02:46<00:37,  7.46s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 26/30 [02:51<00:27,  6.90s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/30 [02:58<00:20,  6.96s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28/30 [03:15<00:19,  9.81s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 29/30 [03:20<00:08,  8.38s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:25<00:00,  6.86s/genome]\u001B[A\n",
      "2025-12-19 10:18:56,637 - INFO - æ‰¹æ¬¡ 1 å®Œæˆ: 30/30 æˆåŠŸ\n",
      "æ€»è¿›åº¦:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 1/2 [03:26<03:26, 206.90s/æ‰¹æ¬¡]2025-12-19 10:18:57,639 - INFO - \n",
      "============================================================\n",
      "2025-12-19 10:18:57,639 - INFO - æ‰¹æ¬¡ 2: 4 ä¸ªåŸºå› ç»„\n",
      "2025-12-19 10:18:57,639 - INFO - ä¿å­˜åˆ°: cglutamicum_downloads\\batch_002/\n",
      "\n",
      "æ‰¹æ¬¡ 2:   0%|                                                             | 0/4 [00:00<?, ?genome/s]\u001B[A\n",
      "æ‰¹æ¬¡ 2:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 1/4 [00:05<00:17,  5.77s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 2/4 [00:11<00:11,  5.82s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 3/4 [00:17<00:05,  5.70s/genome]\u001B[A\n",
      "æ‰¹æ¬¡ 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:22<00:00,  5.64s/genome]\u001B[A\n",
      "2025-12-19 10:19:20,211 - INFO - æ‰¹æ¬¡ 2 å®Œæˆ: 4/4 æˆåŠŸ\n",
      "æ€»è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:49<00:00, 114.74s/æ‰¹æ¬¡]\n",
      "2025-12-19 10:19:20,212 - INFO - \n",
      "============================================================\n",
      "2025-12-19 10:19:20,212 - INFO - ä¸‹è½½å®Œæˆ!\n",
      "2025-12-19 10:19:20,213 - INFO - æ•°æ®æ¥æº: REFSEQ\n",
      "2025-12-19 10:19:20,213 - INFO - æ•°æ®ç±»å‹: genome,protein,gff3,cds\n",
      "2025-12-19 10:19:20,213 - INFO - æ€»åŸºå› ç»„æ•°: 34\n",
      "2025-12-19 10:19:20,214 - INFO - æˆåŠŸä¸‹è½½: 34 (100.0%)\n",
      "2025-12-19 10:19:20,214 - INFO - ä¸‹è½½å¤±è´¥: 0 (0.0%)\n",
      "2025-12-19 10:19:20,214 - INFO - æˆåŠŸæ‰¹æ¬¡: 2/2\n",
      "2025-12-19 10:19:20,215 - INFO - ============================================================\n",
      "2025-12-19 10:19:20,215 - INFO - \n",
      "æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: cglutamicum_downloads/\n",
      "2025-12-19 10:19:20,216 - INFO - \n",
      "ä¸‹è½½çš„æ–‡ä»¶ç±»å‹è¯´æ˜:\n",
      "2025-12-19 10:19:20,216 - INFO -   - genome: åŸºå› ç»„åºåˆ— (*_genomic.fna)\n",
      "2025-12-19 10:19:20,216 - INFO -   - protein: è›‹ç™½è´¨åºåˆ— (*_protein.faa)\n",
      "2025-12-19 10:19:20,216 - INFO -   - gff3: GFF3æ³¨é‡Šæ–‡ä»¶ (*_genomic.gff)\n",
      "2025-12-19 10:19:20,217 - INFO -   - cds: CDSæ ¸è‹·é…¸åºåˆ— (*_cds_from_genomic.fna)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# è§£å‹",
   "id": "eb14cb27fc64b78a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T02:19:25.136055Z",
     "start_time": "2025-12-19T02:19:23.831491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "è§£å‹å’Œæ•´ç†ä»NCBIä¸‹è½½çš„åŸºå› ç»„æ•°æ®\n",
    "åŒ¹é… download_cglutamicum.py çš„è¾“å‡ºç»“æ„\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "class GenomeExtractor:\n",
    "    \"\"\"åŸºå› ç»„æ•°æ®æå–å™¨\"\"\"\n",
    "\n",
    "    def __init__(self, download_dir=\"cglutamicum_downloads\", output_dir=\"cglutamicum_pangenome\"):\n",
    "        self.download_dir = download_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„\n",
    "        self.dirs = {\n",
    "            'genomes': os.path.join(output_dir, 'genomes'),\n",
    "            'proteins': os.path.join(output_dir, 'proteins'),\n",
    "            'cds': os.path.join(output_dir, 'cds'),  # ğŸ‘ˆ æ–°å¢ CDS ç›®å½•\n",
    "            'gff': os.path.join(output_dir, 'gff'),\n",
    "            'metadata': os.path.join(output_dir, 'metadata'),\n",
    "            'logs': os.path.join(output_dir, 'logs')\n",
    "        }\n",
    "\n",
    "        for dir_path in self.dirs.values():\n",
    "            Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # è®¾ç½®æ—¥å¿—\n",
    "        self._setup_logging()\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"è®¾ç½®æ—¥å¿—\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = os.path.join(self.dirs['logs'], f\"extract_{timestamp}.log\")\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file, encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(f\"æ—¥å¿—æ–‡ä»¶: {log_file}\")\n",
    "\n",
    "    def find_all_genome_zips(self):\n",
    "        \"\"\"æŸ¥æ‰¾æ‰€æœ‰ä¸‹è½½çš„åŸºå› ç»„zipæ–‡ä»¶\"\"\"\n",
    "        self.logger.info(f\"æ‰«æä¸‹è½½ç›®å½•: {self.download_dir}\")\n",
    "\n",
    "        genome_zips = []\n",
    "\n",
    "        # éå†æ‰€æœ‰batchç›®å½•\n",
    "        if not os.path.exists(self.download_dir):\n",
    "            self.logger.error(f\"ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {self.download_dir}\")\n",
    "            return []\n",
    "\n",
    "        for item in os.listdir(self.download_dir):\n",
    "            item_path = os.path.join(self.download_dir, item)\n",
    "\n",
    "            # æŸ¥æ‰¾ batch_xxx ç›®å½•\n",
    "            if os.path.isdir(item_path) and item.startswith('batch_'):\n",
    "                # åœ¨batchç›®å½•ä¸­æŸ¥æ‰¾æ‰€æœ‰.zipæ–‡ä»¶\n",
    "                for zip_file in os.listdir(item_path):\n",
    "                    if zip_file.endswith('.zip'):\n",
    "                        zip_path = os.path.join(item_path, zip_file)\n",
    "                        # æå–accessionå·\n",
    "                        accession = zip_file.replace('.zip', '')\n",
    "                        genome_zips.append({\n",
    "                            'accession': accession,\n",
    "                            'path': zip_path,\n",
    "                            'batch': item\n",
    "                        })\n",
    "\n",
    "        self.logger.info(f\"æ‰¾åˆ° {len(genome_zips)} ä¸ªåŸºå› ç»„æ–‡ä»¶\")\n",
    "        return genome_zips\n",
    "\n",
    "    def extract_single_genome(self, genome_info):\n",
    "        \"\"\"\n",
    "        è§£å‹å•ä¸ªåŸºå› ç»„å¹¶æå–å…³é”®æ–‡ä»¶\n",
    "        è¿”å›: (æˆåŠŸ, å…ƒæ•°æ®å­—å…¸)\n",
    "        \"\"\"\n",
    "        accession = genome_info['accession']\n",
    "        zip_path = genome_info['path']\n",
    "\n",
    "        # æ£€æŸ¥æ–‡ä»¶å¤§å°\n",
    "        if os.path.getsize(zip_path) < 1024:\n",
    "            self.logger.warning(f\"æ–‡ä»¶è¿‡å°,å¯èƒ½æŸå: {accession}\")\n",
    "            return False, None\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # æŸ¥æ‰¾å¹¶æå–æ–‡ä»¶\n",
    "                extracted_files = {\n",
    "                    'genome': None,  # .fna\n",
    "                    'protein': None, # .faa\n",
    "                    'cds': None,     # _cds_from_genomic.fna  ğŸ‘ˆ æ–°å¢\n",
    "                    'gff': None,     # .gff\n",
    "                    'jsonl': None    # data_report.jsonl\n",
    "                }\n",
    "\n",
    "                for file_info in zip_ref.filelist:\n",
    "                    filename = file_info.filename\n",
    "\n",
    "                    # CDS æ ¸è‹·é…¸åºåˆ—æ–‡ä»¶ ğŸ‘ˆ æ–°å¢ï¼ˆæ”¾åœ¨åŸºå› ç»„ä¹‹å‰åˆ¤æ–­ï¼‰\n",
    "                    if 'cds_from_genomic.fna' in filename:\n",
    "                        target = os.path.join(self.dirs['cds'], f\"{accession}_cds.fna\")\n",
    "                        with zip_ref.open(filename) as source:\n",
    "                            with open(target, 'wb') as dest:\n",
    "                                shutil.copyfileobj(source, dest)\n",
    "                        extracted_files['cds'] = target\n",
    "\n",
    "                    # åŸºå› ç»„åºåˆ—æ–‡ä»¶ï¼ˆæ’é™¤ cds å’Œ rnaï¼‰\n",
    "                    elif (filename.endswith('.fna') or filename.endswith('_genomic.fna')) and \\\n",
    "                         'cds_from_genomic' not in filename and 'rna_from_genomic' not in filename:\n",
    "                        target = os.path.join(self.dirs['genomes'], f\"{accession}.fna\")\n",
    "                        with zip_ref.open(filename) as source:\n",
    "                            with open(target, 'wb') as dest:\n",
    "                                shutil.copyfileobj(source, dest)\n",
    "                        extracted_files['genome'] = target\n",
    "\n",
    "                    # è›‹ç™½è´¨åºåˆ—æ–‡ä»¶\n",
    "                    elif filename.endswith('.faa') or filename.endswith('_protein.faa'):\n",
    "                        target = os.path.join(self.dirs['proteins'], f\"{accession}.faa\")\n",
    "                        with zip_ref.open(filename) as source:\n",
    "                            with open(target, 'wb') as dest:\n",
    "                                shutil.copyfileobj(source, dest)\n",
    "                        extracted_files['protein'] = target\n",
    "\n",
    "                    # GFFæ³¨é‡Šæ–‡ä»¶\n",
    "                    elif filename.endswith('.gff') or filename.endswith('_genomic.gff'):\n",
    "                        target = os.path.join(self.dirs['gff'], f\"{accession}.gff\")\n",
    "                        with zip_ref.open(filename) as source:\n",
    "                            with open(target, 'wb') as dest:\n",
    "                                shutil.copyfileobj(source, dest)\n",
    "                        extracted_files['gff'] = target\n",
    "\n",
    "                    # å…ƒæ•°æ®\n",
    "                    elif 'data_report.jsonl' in filename:\n",
    "                        with zip_ref.open(filename) as source:\n",
    "                            content = source.read().decode('utf-8')\n",
    "                            extracted_files['jsonl'] = content\n",
    "\n",
    "                # è§£æå…ƒæ•°æ®\n",
    "                metadata = self._parse_metadata(accession, extracted_files)\n",
    "\n",
    "                # éªŒè¯å…³é”®æ–‡ä»¶\n",
    "                if not extracted_files['genome']:\n",
    "                    self.logger.warning(f\"ç¼ºå°‘åŸºå› ç»„åºåˆ—æ–‡ä»¶: {accession}\")\n",
    "                    return False, None\n",
    "\n",
    "                return True, metadata\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"è§£å‹å¤±è´¥ {accession}: {str(e)}\")\n",
    "            return False, None\n",
    "\n",
    "    def _parse_metadata(self, accession, extracted_files):\n",
    "        \"\"\"è§£æåŸºå› ç»„å…ƒæ•°æ®\"\"\"\n",
    "        metadata = {\n",
    "            'accession': accession,\n",
    "            'organism': 'Corynebacterium glutamicum',\n",
    "            'has_genome': extracted_files['genome'] is not None,\n",
    "            'has_protein': extracted_files['protein'] is not None,\n",
    "            'has_cds': extracted_files['cds'] is not None,  # ğŸ‘ˆ æ–°å¢\n",
    "            'has_gff': extracted_files['gff'] is not None,\n",
    "        }\n",
    "\n",
    "        # ä»JSONLè§£ææ›´å¤šä¿¡æ¯\n",
    "        if extracted_files['jsonl']:\n",
    "            try:\n",
    "                data = json.loads(extracted_files['jsonl'])\n",
    "                metadata.update({\n",
    "                    'organism_name': data.get('organism', {}).get('organismName', 'N/A'),\n",
    "                    'strain': data.get('organism', {}).get('infraspecificNames', {}).get('strain', 'N/A'),\n",
    "                    'assembly_level': data.get('assemblyInfo', {}).get('assemblyLevel', 'N/A'),\n",
    "                    'contig_n50': data.get('assemblyStats', {}).get('contigN50', 'N/A'),\n",
    "                    'total_sequence_length': data.get('assemblyStats', {}).get('totalSequenceLength', 'N/A'),\n",
    "                    'number_of_contigs': data.get('assemblyStats', {}).get('numberOfContigs', 'N/A'),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"æ— æ³•è§£æå…ƒæ•°æ®: {accession}\")\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def extract_all_genomes(self, genome_zips):\n",
    "        \"\"\"æå–æ‰€æœ‰åŸºå› ç»„\"\"\"\n",
    "        self.logger.info(\"å¼€å§‹æå–åŸºå› ç»„æ–‡ä»¶...\")\n",
    "\n",
    "        successful = 0\n",
    "        failed = []\n",
    "        all_metadata = []\n",
    "\n",
    "        # ä½¿ç”¨è¿›åº¦æ¡\n",
    "        for genome_info in tqdm(genome_zips, desc=\"è§£å‹åŸºå› ç»„\", unit=\"genome\"):\n",
    "            success, metadata = self.extract_single_genome(genome_info)\n",
    "\n",
    "            if success:\n",
    "                successful += 1\n",
    "                if metadata:\n",
    "                    all_metadata.append(metadata)\n",
    "            else:\n",
    "                failed.append(genome_info['accession'])\n",
    "\n",
    "        self.logger.info(f\"\\næå–å®Œæˆ: {successful}/{len(genome_zips)} æˆåŠŸ\")\n",
    "\n",
    "        if failed:\n",
    "            self.logger.warning(f\"å¤±è´¥: {len(failed)} ä¸ª\")\n",
    "            failed_file = os.path.join(self.dirs['logs'], 'extraction_failed.txt')\n",
    "            with open(failed_file, 'w') as f:\n",
    "                f.write('\\n'.join(failed))\n",
    "            self.logger.info(f\"å¤±è´¥åˆ—è¡¨: {failed_file}\")\n",
    "\n",
    "        # ä¿å­˜å…ƒæ•°æ®æ‘˜è¦\n",
    "        self._save_metadata_summary(all_metadata)\n",
    "\n",
    "        return successful, failed\n",
    "\n",
    "    def _save_metadata_summary(self, metadata_list):\n",
    "        \"\"\"ä¿å­˜å…ƒæ•°æ®æ‘˜è¦\"\"\"\n",
    "        if not metadata_list:\n",
    "            return\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.DataFrame(metadata_list)\n",
    "\n",
    "        # ä¿å­˜ä¸ºCSV\n",
    "        csv_file = os.path.join(self.dirs['metadata'], 'genomes_summary.csv')\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        self.logger.info(f\"å…ƒæ•°æ®æ‘˜è¦: {csv_file}\")\n",
    "\n",
    "        # æ‰“å°ç»Ÿè®¡\n",
    "        self.logger.info(\"\\n=== åŸºå› ç»„ç»Ÿè®¡ ===\")\n",
    "        self.logger.info(f\"æ€»æ•°: {len(df)}\")\n",
    "        self.logger.info(f\"æœ‰GFFæ³¨é‡Š: {df['has_gff'].sum()}\")\n",
    "        self.logger.info(f\"æœ‰è›‹ç™½åºåˆ—: {df['has_protein'].sum()}\")\n",
    "        self.logger.info(f\"æœ‰CDSåºåˆ—: {df['has_cds'].sum()}\")  # ğŸ‘ˆ æ–°å¢\n",
    "\n",
    "        if 'assembly_level' in df.columns:\n",
    "            self.logger.info(\"\\nç»„è£…æ°´å¹³åˆ†å¸ƒ:\")\n",
    "            for level, count in df['assembly_level'].value_counts().items():\n",
    "                self.logger.info(f\"  {level}: {count}\")\n",
    "\n",
    "    def generate_file_lists(self):\n",
    "        \"\"\"ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨(ç”¨äºåç»­åˆ†æ)\"\"\"\n",
    "        lists = {\n",
    "            'genomes': [],\n",
    "            'proteins': [],\n",
    "            'cds': [],  # ğŸ‘ˆ æ–°å¢\n",
    "            'gff': []\n",
    "        }\n",
    "\n",
    "        for key, dir_path in [('genomes', self.dirs['genomes']),\n",
    "                              ('proteins', self.dirs['proteins']),\n",
    "                              ('cds', self.dirs['cds']),  # ğŸ‘ˆ æ–°å¢\n",
    "                              ('gff', self.dirs['gff'])]:\n",
    "            if os.path.exists(dir_path):\n",
    "                files = sorted([f for f in os.listdir(dir_path)\n",
    "                              if not f.startswith('.')])\n",
    "                lists[key] = [os.path.join(dir_path, f) for f in files]\n",
    "\n",
    "        # ä¿å­˜åˆ—è¡¨\n",
    "        for key, file_list in lists.items():\n",
    "            list_file = os.path.join(self.dirs['metadata'], f'{key}_list.txt')\n",
    "            with open(list_file, 'w') as f:\n",
    "                f.write('\\n'.join(file_list))\n",
    "            self.logger.info(f\"{key.capitalize()} åˆ—è¡¨: {list_file} ({len(file_list)} ä¸ªæ–‡ä»¶)\")\n",
    "\n",
    "        return lists\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"æ‰“å°æ‘˜è¦\"\"\"\n",
    "        self.logger.info(\"\\n\" + \"=\"*60)\n",
    "        self.logger.info(\"æ–‡ä»¶æ•´ç†å®Œæˆ!\")\n",
    "        self.logger.info(f\"è¾“å‡ºç›®å½•: {self.output_dir}/\")\n",
    "        self.logger.info(\"\\nç›®å½•ç»“æ„:\")\n",
    "        self.logger.info(f\"  genomes/   - åŸºå› ç»„åºåˆ— (.fna)\")\n",
    "        self.logger.info(f\"  proteins/  - è›‹ç™½è´¨åºåˆ— (.faa)\")\n",
    "        self.logger.info(f\"  cds/       - CDSæ ¸è‹·é…¸åºåˆ— (.fna)\")  # ğŸ‘ˆ æ–°å¢\n",
    "        self.logger.info(f\"  gff/       - åŸºå› æ³¨é‡Š (.gff)\")\n",
    "        self.logger.info(f\"  metadata/  - å…ƒæ•°æ®å’Œæ–‡ä»¶åˆ—è¡¨\")\n",
    "        self.logger.info(f\"  logs/      - æ—¥å¿—æ–‡ä»¶\")\n",
    "        self.logger.info(\"=\"*60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # é…ç½®\n",
    "    DOWNLOAD_DIR = \"cglutamicum_downloads\"  # ä¸‹è½½è„šæœ¬çš„è¾“å‡ºç›®å½•\n",
    "    OUTPUT_DIR = \"cglutamicum_pangenome\"     # æ•´ç†åçš„è¾“å‡ºç›®å½•\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"C. glutamicum åŸºå› ç»„æ•°æ®æå–å™¨\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # æ£€æŸ¥ä¸‹è½½ç›®å½•\n",
    "    if not os.path.exists(DOWNLOAD_DIR):\n",
    "        print(f\"é”™è¯¯: ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {DOWNLOAD_DIR}\")\n",
    "        print(\"è¯·ç¡®ä¿å·²è¿è¡Œä¸‹è½½è„šæœ¬å¹¶å®Œæˆä¸‹è½½\")\n",
    "        return\n",
    "\n",
    "    # åˆ›å»ºæå–å™¨\n",
    "    extractor = GenomeExtractor(DOWNLOAD_DIR, OUTPUT_DIR)\n",
    "\n",
    "    # æŸ¥æ‰¾æ‰€æœ‰åŸºå› ç»„æ–‡ä»¶\n",
    "    genome_zips = extractor.find_all_genome_zips()\n",
    "\n",
    "    if not genome_zips:\n",
    "        print(\"æœªæ‰¾åˆ°åŸºå› ç»„æ–‡ä»¶!\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\næ‰¾åˆ° {len(genome_zips)} ä¸ªåŸºå› ç»„æ–‡ä»¶\")\n",
    "\n",
    "    # æå–æ‰€æœ‰åŸºå› ç»„\n",
    "    successful, failed = extractor.extract_all_genomes(genome_zips)\n",
    "\n",
    "    # ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨\n",
    "    extractor.generate_file_lists()\n",
    "\n",
    "    # æ‰“å°æ‘˜è¦\n",
    "    extractor.print_summary()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "cffc0d0ee48ec710",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 10:19:23,848 - INFO - æ—¥å¿—æ–‡ä»¶: cglutamicum_pangenome\\logs\\extract_20251219_101923.log\n",
      "2025-12-19 10:19:23,849 - INFO - æ‰«æä¸‹è½½ç›®å½•: cglutamicum_downloads\n",
      "2025-12-19 10:19:23,850 - INFO - æ‰¾åˆ° 34 ä¸ªåŸºå› ç»„æ–‡ä»¶\n",
      "2025-12-19 10:19:23,850 - INFO - å¼€å§‹æå–åŸºå› ç»„æ–‡ä»¶...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "C. glutamicum åŸºå› ç»„æ•°æ®æå–å™¨\n",
      "============================================================\n",
      "\n",
      "æ‰¾åˆ° 34 ä¸ªåŸºå› ç»„æ–‡ä»¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è§£å‹åŸºå› ç»„: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:00<00:00, 39.91genome/s]\n",
      "2025-12-19 10:19:24,704 - INFO - \n",
      "æå–å®Œæˆ: 34/34 æˆåŠŸ\n",
      "2025-12-19 10:19:25,116 - INFO - å…ƒæ•°æ®æ‘˜è¦: cglutamicum_pangenome\\metadata\\genomes_summary.csv\n",
      "2025-12-19 10:19:25,116 - INFO - \n",
      "=== åŸºå› ç»„ç»Ÿè®¡ ===\n",
      "2025-12-19 10:19:25,117 - INFO - æ€»æ•°: 34\n",
      "2025-12-19 10:19:25,119 - INFO - æœ‰GFFæ³¨é‡Š: 34\n",
      "2025-12-19 10:19:25,119 - INFO - æœ‰è›‹ç™½åºåˆ—: 34\n",
      "2025-12-19 10:19:25,120 - INFO - æœ‰CDSåºåˆ—: 34\n",
      "2025-12-19 10:19:25,121 - INFO - \n",
      "ç»„è£…æ°´å¹³åˆ†å¸ƒ:\n",
      "2025-12-19 10:19:25,124 - INFO -   Complete Genome: 34\n",
      "2025-12-19 10:19:25,126 - INFO - Genomes åˆ—è¡¨: cglutamicum_pangenome\\metadata\\genomes_list.txt (34 ä¸ªæ–‡ä»¶)\n",
      "2025-12-19 10:19:25,127 - INFO - Proteins åˆ—è¡¨: cglutamicum_pangenome\\metadata\\proteins_list.txt (34 ä¸ªæ–‡ä»¶)\n",
      "2025-12-19 10:19:25,128 - INFO - Cds åˆ—è¡¨: cglutamicum_pangenome\\metadata\\cds_list.txt (34 ä¸ªæ–‡ä»¶)\n",
      "2025-12-19 10:19:25,129 - INFO - Gff åˆ—è¡¨: cglutamicum_pangenome\\metadata\\gff_list.txt (34 ä¸ªæ–‡ä»¶)\n",
      "2025-12-19 10:19:25,129 - INFO - \n",
      "============================================================\n",
      "2025-12-19 10:19:25,129 - INFO - æ–‡ä»¶æ•´ç†å®Œæˆ!\n",
      "2025-12-19 10:19:25,130 - INFO - è¾“å‡ºç›®å½•: cglutamicum_pangenome/\n",
      "2025-12-19 10:19:25,130 - INFO - \n",
      "ç›®å½•ç»“æ„:\n",
      "2025-12-19 10:19:25,131 - INFO -   genomes/   - åŸºå› ç»„åºåˆ— (.fna)\n",
      "2025-12-19 10:19:25,131 - INFO -   proteins/  - è›‹ç™½è´¨åºåˆ— (.faa)\n",
      "2025-12-19 10:19:25,132 - INFO -   cds/       - CDSæ ¸è‹·é…¸åºåˆ— (.fna)\n",
      "2025-12-19 10:19:25,132 - INFO -   gff/       - åŸºå› æ³¨é‡Š (.gff)\n",
      "2025-12-19 10:19:25,132 - INFO -   metadata/  - å…ƒæ•°æ®å’Œæ–‡ä»¶åˆ—è¡¨\n",
      "2025-12-19 10:19:25,133 - INFO -   logs/      - æ—¥å¿—æ–‡ä»¶\n",
      "2025-12-19 10:19:25,133 - INFO - ============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# å»ºæ ‘-åœ¨WSLä¸­è¿è¡Œ",
   "id": "e05f292eab640250"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "conda info --envs\n",
    "cd /mnt/d/pycharm/iCG67_zxy/\n",
    "python script/run_complete_pipeline.py \\\n",
    "    -i cglutamicum_pangenome/genomes \\\n",
    "    -o snp_tree_results_docker \\\n",
    "    -t 8 \\\n",
    "    --species \"Corynebacterium glutamicum\" \\\n",
    "    --ani-threshold 100\n",
    "\"\"\""
   ],
   "id": "50fb78743985f4b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# è¯†åˆ«åŒæºåŸºå› -OrthoFinder in WSL",
   "id": "381aa64a692c4050"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# conda create -n orthofinder_py310 python=3.10 -y\n",
    "# conda activate orthofinder_py310\n",
    "# conda install -c bioconda -c conda-forge orthofinder -y\n",
    "# cd /mnt/d/pycharm/iCG67_zxy/\n",
    "#\n",
    "#è¿è¡Œ OrthoFinder\n",
    "'''\n",
    "  orthofinder -f cglutamicum_pangenome/proteins \\\n",
    "              -t 14 \\\n",
    "              -a 8 \\\n",
    "              -S diamond \\\n",
    "              -o orthofinder_results\n",
    "'''"
   ],
   "id": "4795bebe7786db07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# å¯¹æ˜ locu_tag",
   "id": "ed7c4a6311df3dc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# ã€åªéœ€ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„ã€‘\n",
    "# ============================================================\n",
    "INPUT_FILE = \"orthofinder_results/Results_Dec14/Orthogroups/Orthogroups.tsv\"\n",
    "GENOMES_DIR = \"cglutamicum_pangenome/genomes\"\n",
    "CDS_DIR = \"cglutamicum_pangenome/cds\" # å¦‚æœCDSæ–‡ä»¶åœ¨å•ç‹¬ç›®å½•åˆ™å¡«å†™è·¯å¾„\n",
    "OUTPUT_FILE = \"converted_orthogroups.tsv\"\n",
    "\n",
    "# ============================================================\n",
    "# ä»¥ä¸‹ä»£ç æ— éœ€ä¿®æ”¹\n",
    "# ============================================================\n",
    "import os, re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_fna_for_species_name(fna_file):\n",
    "    with open(fna_file, 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "    if not first_line.startswith('>'): return None\n",
    "    parts = first_line[1:].split(' ', 1)\n",
    "    if len(parts) < 2: return None\n",
    "    desc = re.sub(r',?\\s*(complete\\s+)?(sequence|genome|chromosome).*$', '', parts[1], flags=re.IGNORECASE)\n",
    "    return desc.strip() if desc.strip() else None\n",
    "\n",
    "def parse_cds_file(cds_file):\n",
    "    wp_to_locus = {}\n",
    "    with open(cds_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('>'): continue\n",
    "            wp_match = re.search(r'(WP_\\d+\\.\\d+)', line)\n",
    "            locus_match = re.search(r'\\[locus_tag=([^\\]]+)\\]', line)\n",
    "            if wp_match and locus_match:\n",
    "                wp_to_locus[wp_match.group(1)] = locus_match.group(1)\n",
    "    return wp_to_locus\n",
    "\n",
    "def convert_cell(cell_value, wp_to_locus):\n",
    "    if pd.isna(cell_value) or str(cell_value).strip() in ['', '<null>']: return cell_value\n",
    "    ids = re.split(r',\\s*', str(cell_value))\n",
    "    return ', '.join([wp_to_locus.get(i.strip(), i.strip()) for i in ids if i.strip()])\n",
    "\n",
    "# æŸ¥æ‰¾æ–‡ä»¶\n",
    "genomes_dir = Path(GENOMES_DIR)\n",
    "cds_dir = Path(CDS_DIR) if CDS_DIR else genomes_dir\n",
    "\n",
    "fna_files = {f.stem: f for f in genomes_dir.glob('*.fna')}\n",
    "print(f\"æ‰¾åˆ° {len(fna_files)} ä¸ª.fnaæ–‡ä»¶\")\n",
    "\n",
    "cds_files = {}\n",
    "for pattern in ['*_cds_from_genomic.fna', '*.faa', '*_cds.fna']:\n",
    "    for f in cds_dir.glob(pattern):\n",
    "        for gcf_id in fna_files:\n",
    "            if gcf_id in str(f):\n",
    "                cds_files[gcf_id] = f\n",
    "print(f\"æ‰¾åˆ° {len(cds_files)} ä¸ªCDSæ–‡ä»¶\")\n",
    "\n",
    "# ã€å…³é”®ä¿®æ­£ã€‘ä¸ºæ¯ä¸ªèŒæ ªå•ç‹¬æ„å»ºæ˜ å°„\n",
    "gcf_to_species = {}\n",
    "gcf_to_wp_locus = {}  # {gcf_id: {wp_id: locus_tag}}\n",
    "\n",
    "for gcf_id, fna_path in fna_files.items():\n",
    "    species = parse_fna_for_species_name(fna_path)\n",
    "    gcf_to_species[gcf_id] = species if species else gcf_id\n",
    "\n",
    "    if gcf_id in cds_files:\n",
    "        gcf_to_wp_locus[gcf_id] = parse_cds_file(cds_files[gcf_id])\n",
    "        print(f\"{gcf_id} -> {gcf_to_species[gcf_id]} ({len(gcf_to_wp_locus[gcf_id])} æ˜ å°„)\")\n",
    "    else:\n",
    "        gcf_to_wp_locus[gcf_id] = {}\n",
    "        print(f\"{gcf_id} -> {gcf_to_species[gcf_id]} [æ— CDSæ–‡ä»¶]\")\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_csv(INPUT_FILE, sep='\\t')\n",
    "print(f\"\\nåŸå§‹æ•°æ®: {len(df)} è¡Œ x {len(df.columns)} åˆ—\")\n",
    "\n",
    "# ä¿å­˜åŸå§‹åˆ—ååˆ°GCFçš„æ˜ å°„\n",
    "original_cols = list(df.columns)\n",
    "\n",
    "# ã€å…³é”®ã€‘è½¬æ¢æ¯åˆ—æ—¶ä½¿ç”¨å¯¹åº”èŒæ ªçš„æ˜ å°„è¡¨\n",
    "for col_idx, gcf_id in enumerate(original_cols[1:], 1):\n",
    "    strain_mapping = gcf_to_wp_locus.get(gcf_id, {})\n",
    "    df.iloc[:, col_idx] = df.iloc[:, col_idx].apply(lambda x: convert_cell(x, strain_mapping))\n",
    "\n",
    "# è½¬æ¢åˆ—å\n",
    "df.columns = ['Orthogroup'] + [gcf_to_species.get(c, c) for c in original_cols[1:]]\n",
    "\n",
    "# ä¿å­˜\n",
    "df.to_csv(OUTPUT_FILE, sep='\\t', index=False)\n",
    "print(f\"\\nâœ… å®Œæˆ! å·²ä¿å­˜åˆ°: {OUTPUT_FILE}\")"
   ],
   "id": "9d84e1c2d998ce94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# åˆ†æåŒæºä»¥åŠéåŒæºåŸºå› ",
   "id": "cc9d7645f5fb608e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T09:57:12.818474Z",
     "start_time": "2025-12-20T09:57:12.077964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "OrthoFinder Ortholog Analysis - Streamlined Output\n",
    "Analysis of orthologous genes between C. glutamicum ATCC 14067 and ATCC 13032\n",
    "\n",
    "Outputs:\n",
    "1. Comprehensive text report (analysis_report.txt)\n",
    "2. Single Excel file with multiple sheets (ortholog_data.xlsx)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_orthogroups(filepath, strain1_keyword=\"14067\", strain2_keyword=\"13032\"):\n",
    "    \"\"\"\n",
    "    Parse OrthoFinder Orthogroups.tsv file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, sep='\\t')\n",
    "\n",
    "    # Find columns for each strain\n",
    "    strain1_cols = [col for col in df.columns if strain1_keyword in col]\n",
    "    strain2_cols = [col for col in df.columns if strain2_keyword in col]\n",
    "\n",
    "    if not strain1_cols or not strain2_cols:\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        raise ValueError(f\"Could not find columns containing '{strain1_keyword}' or '{strain2_keyword}'\")\n",
    "\n",
    "    strain1_col = strain1_cols[0]\n",
    "    strain2_col = strain2_cols[0]\n",
    "\n",
    "    print(f\"Analyzing: {strain1_col} vs {strain2_col}\")\n",
    "\n",
    "    # Initialize\n",
    "    strain1_genes = set()\n",
    "    strain2_genes = set()\n",
    "    shared_orthogroups = []\n",
    "    strain1_only_orthogroups = []\n",
    "    strain2_only_orthogroups = []\n",
    "\n",
    "    # Process each orthogroup\n",
    "    for idx, row in df.iterrows():\n",
    "        og = row['Orthogroup']\n",
    "\n",
    "        s1_genes_str = str(row[strain1_col]) if pd.notna(row[strain1_col]) else \"\"\n",
    "        s2_genes_str = str(row[strain2_col]) if pd.notna(row[strain2_col]) else \"\"\n",
    "\n",
    "        s1_genes = set()\n",
    "        s2_genes = set()\n",
    "\n",
    "        if s1_genes_str and s1_genes_str != 'nan':\n",
    "            s1_genes = set([g.strip() for g in s1_genes_str.replace(',', ' ').split() if g.strip()])\n",
    "\n",
    "        if s2_genes_str and s2_genes_str != 'nan':\n",
    "            s2_genes = set([g.strip() for g in s2_genes_str.replace(',', ' ').split() if g.strip()])\n",
    "\n",
    "        strain1_genes.update(s1_genes)\n",
    "        strain2_genes.update(s2_genes)\n",
    "\n",
    "        has_s1 = len(s1_genes) > 0\n",
    "        has_s2 = len(s2_genes) > 0\n",
    "\n",
    "        if has_s1 and has_s2:\n",
    "            shared_orthogroups.append({\n",
    "                'orthogroup': og,\n",
    "                'strain1_genes': s1_genes,\n",
    "                'strain2_genes': s2_genes,\n",
    "                'strain1_count': len(s1_genes),\n",
    "                'strain2_count': len(s2_genes)\n",
    "            })\n",
    "        elif has_s1:\n",
    "            strain1_only_orthogroups.append({\n",
    "                'orthogroup': og,\n",
    "                'genes': s1_genes,\n",
    "                'count': len(s1_genes)\n",
    "            })\n",
    "        elif has_s2:\n",
    "            strain2_only_orthogroups.append({\n",
    "                'orthogroup': og,\n",
    "                'genes': s2_genes,\n",
    "                'count': len(s2_genes)\n",
    "            })\n",
    "\n",
    "    shared_genes_s1 = sum([og['strain1_count'] for og in shared_orthogroups])\n",
    "    shared_genes_s2 = sum([og['strain2_count'] for og in shared_orthogroups])\n",
    "    unique_genes_s1 = sum([og['count'] for og in strain1_only_orthogroups])\n",
    "    unique_genes_s2 = sum([og['count'] for og in strain2_only_orthogroups])\n",
    "\n",
    "    return {\n",
    "        'strain1_name': strain1_col,\n",
    "        'strain2_name': strain2_col,\n",
    "        'strain1_total_genes': len(strain1_genes),\n",
    "        'strain2_total_genes': len(strain2_genes),\n",
    "        'shared_orthogroups': shared_orthogroups,\n",
    "        'strain1_only_orthogroups': strain1_only_orthogroups,\n",
    "        'strain2_only_orthogroups': strain2_only_orthogroups,\n",
    "        'num_shared_orthogroups': len(shared_orthogroups),\n",
    "        'num_strain1_only_orthogroups': len(strain1_only_orthogroups),\n",
    "        'num_strain2_only_orthogroups': len(strain2_only_orthogroups),\n",
    "        'shared_genes_s1': shared_genes_s1,\n",
    "        'shared_genes_s2': shared_genes_s2,\n",
    "        'unique_genes_s1': unique_genes_s1,\n",
    "        'unique_genes_s2': unique_genes_s2\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_comprehensive_report(results, output_path):\n",
    "    \"\"\"\n",
    "    Generate comprehensive analysis report with data preview\n",
    "    \"\"\"\n",
    "    s1_short = \"ATCC 14067\"\n",
    "    s2_short = \"ATCC 13032\"\n",
    "\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"ORTHOFINDER COMPARATIVE ANALYSIS REPORT\")\n",
    "    report.append(\"Corynebacterium glutamicum ATCC 14067 vs ATCC 13032\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Summary Statistics\n",
    "    report.append(\"1. SUMMARY STATISTICS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"{'Metric':<50} {'Value':>15}\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"{'Total genes in ' + s1_short:<50} {results['strain1_total_genes']:>15,}\")\n",
    "    report.append(f\"{'Total genes in ' + s2_short:<50} {results['strain2_total_genes']:>15,}\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Orthogroup Analysis\n",
    "    report.append(\"2. ORTHOGROUP ANALYSIS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    total_ogs = (results['num_shared_orthogroups'] +\n",
    "                 results['num_strain1_only_orthogroups'] +\n",
    "                 results['num_strain2_only_orthogroups'])\n",
    "\n",
    "    report.append(f\"{'Total orthogroups analyzed:':<50} {total_ogs:>15,}\")\n",
    "    report.append(f\"{'Shared orthogroups (core genome):':<50} {results['num_shared_orthogroups']:>15,}\")\n",
    "    report.append(f\"{'Orthogroups unique to ' + s1_short + ':':<50} {results['num_strain1_only_orthogroups']:>15,}\")\n",
    "    report.append(f\"{'Orthogroups unique to ' + s2_short + ':':<50} {results['num_strain2_only_orthogroups']:>15,}\")\n",
    "\n",
    "    if total_ogs > 0:\n",
    "        report.append(f\"{'Shared orthogroups (%):':<50} {results['num_shared_orthogroups']/total_ogs*100:>14.1f}%\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Gene Distribution\n",
    "    report.append(\"3. GENE DISTRIBUTION\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"{'Core genes (' + s1_short + '):':<50} {results['shared_genes_s1']:>15,}\")\n",
    "    report.append(f\"{'Core genes (' + s2_short + '):':<50} {results['shared_genes_s2']:>15,}\")\n",
    "    report.append(f\"{'Strain-specific genes (' + s1_short + '):':<50} {results['unique_genes_s1']:>15,}\")\n",
    "    report.append(f\"{'Strain-specific genes (' + s2_short + '):':<50} {results['unique_genes_s2']:>15,}\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Core Genome Analysis\n",
    "    if results['strain1_total_genes'] > 0:\n",
    "        core_pct_s1 = results['shared_genes_s1'] / results['strain1_total_genes'] * 100\n",
    "    else:\n",
    "        core_pct_s1 = 0\n",
    "\n",
    "    if results['strain2_total_genes'] > 0:\n",
    "        core_pct_s2 = results['shared_genes_s2'] / results['strain2_total_genes'] * 100\n",
    "    else:\n",
    "        core_pct_s2 = 0\n",
    "\n",
    "    report.append(\"4. CORE GENOME ANALYSIS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"{'Core genome proportion (' + s1_short + '):':<50} {core_pct_s1:>14.1f}%\")\n",
    "    report.append(f\"{'Core genome proportion (' + s2_short + '):':<50} {core_pct_s2:>14.1f}%\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Data Preview - Shared Orthogroups\n",
    "    report.append(\"5. DATA PREVIEW - SHARED ORTHOGROUPS (First 10)\")\n",
    "    report.append(\"-\" * 80)\n",
    "    for i, og in enumerate(results['shared_orthogroups'][:10], 1):\n",
    "        report.append(f\"\\n{i}. Orthogroup: {og['orthogroup']}\")\n",
    "        report.append(f\"   {s1_short} genes ({og['strain1_count']}): {', '.join(sorted(list(og['strain1_genes'])[:5]))}\")\n",
    "        if og['strain1_count'] > 5:\n",
    "            report.append(f\"      ... and {og['strain1_count'] - 5} more\")\n",
    "        report.append(f\"   {s2_short} genes ({og['strain2_count']}): {', '.join(sorted(list(og['strain2_genes'])[:5]))}\")\n",
    "        if og['strain2_count'] > 5:\n",
    "            report.append(f\"      ... and {og['strain2_count'] - 5} more\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Data Preview - Strain-specific\n",
    "    report.append(\"6. DATA PREVIEW - STRAIN-SPECIFIC ORTHOGROUPS (First 5 each)\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"\\n{s1_short}-specific:\")\n",
    "    for i, og in enumerate(results['strain1_only_orthogroups'][:5], 1):\n",
    "        genes_list = ', '.join(sorted(list(og['genes'])[:5]))\n",
    "        report.append(f\"   {i}. {og['orthogroup']} ({og['count']} genes): {genes_list}\")\n",
    "        if og['count'] > 5:\n",
    "            report.append(f\"      ... and {og['count'] - 5} more\")\n",
    "\n",
    "    report.append(f\"\\n{s2_short}-specific:\")\n",
    "    for i, og in enumerate(results['strain2_only_orthogroups'][:5], 1):\n",
    "        genes_list = ', '.join(sorted(list(og['genes'])[:5]))\n",
    "        report.append(f\"   {i}. {og['orthogroup']} ({og['count']} genes): {genes_list}\")\n",
    "        if og['count'] > 5:\n",
    "            report.append(f\"      ... and {og['count'] - 5} more\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # File Reference\n",
    "    report.append(\"7. OUTPUT FILES\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(\"Complete gene lists exported to: ortholog_data.xlsx\")\n",
    "    report.append(\"  - Sheet 1: Summary Statistics\")\n",
    "    report.append(\"  - Sheet 2: Shared Orthogroups\")\n",
    "    report.append(\"  - Sheet 3: ATCC 14067-specific Orthogroups\")\n",
    "    report.append(\"  - Sheet 4: ATCC 13032-specific Orthogroups\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"End of Report\")\n",
    "    report.append(\"=\" * 80)\n",
    "\n",
    "    report_text = \"\\n\".join(report)\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    print(report_text)\n",
    "    print(f\"\\nReport saved to: {output_path}\")\n",
    "\n",
    "    return report_text\n",
    "\n",
    "\n",
    "def export_to_excel(results, output_path):\n",
    "    \"\"\"\n",
    "    Export all data to a single Excel file with multiple sheets\n",
    "    \"\"\"\n",
    "    s1_short = \"ATCC_14067\"\n",
    "    s2_short = \"ATCC_13032\"\n",
    "\n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        # Sheet 1: Summary Statistics\n",
    "        summary_data = {\n",
    "            'Metric': [\n",
    "                'Total Genes (ATCC 14067)',\n",
    "                'Total Genes (ATCC 13032)',\n",
    "                '',\n",
    "                'Total Orthogroups',\n",
    "                'Shared Orthogroups',\n",
    "                'ATCC 14067-specific Orthogroups',\n",
    "                'ATCC 13032-specific Orthogroups',\n",
    "                '',\n",
    "                'Core Genes in ATCC 14067',\n",
    "                'Core Genes in ATCC 13032',\n",
    "                'Unique Genes in ATCC 14067',\n",
    "                'Unique Genes in ATCC 13032',\n",
    "                '',\n",
    "                'Core Genome % (ATCC 14067)',\n",
    "                'Core Genome % (ATCC 13032)'\n",
    "            ],\n",
    "            'Value': [\n",
    "                results['strain1_total_genes'],\n",
    "                results['strain2_total_genes'],\n",
    "                '',\n",
    "                results['num_shared_orthogroups'] + results['num_strain1_only_orthogroups'] + results['num_strain2_only_orthogroups'],\n",
    "                results['num_shared_orthogroups'],\n",
    "                results['num_strain1_only_orthogroups'],\n",
    "                results['num_strain2_only_orthogroups'],\n",
    "                '',\n",
    "                results['shared_genes_s1'],\n",
    "                results['shared_genes_s2'],\n",
    "                results['unique_genes_s1'],\n",
    "                results['unique_genes_s2'],\n",
    "                '',\n",
    "                f\"{results['shared_genes_s1']/results['strain1_total_genes']*100:.1f}%\" if results['strain1_total_genes'] > 0 else 'N/A',\n",
    "                f\"{results['shared_genes_s2']/results['strain2_total_genes']*100:.1f}%\" if results['strain2_total_genes'] > 0 else 'N/A'\n",
    "            ]\n",
    "        }\n",
    "        df_summary = pd.DataFrame(summary_data)\n",
    "        df_summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "        # Sheet 2: Shared Orthogroups\n",
    "        shared_data = []\n",
    "        for og in results['shared_orthogroups']:\n",
    "            shared_data.append({\n",
    "                'Orthogroup': og['orthogroup'],\n",
    "                f'{s1_short}_Genes': ', '.join(sorted(og['strain1_genes'])),\n",
    "                f'{s1_short}_Count': og['strain1_count'],\n",
    "                f'{s2_short}_Genes': ', '.join(sorted(og['strain2_genes'])),\n",
    "                f'{s2_short}_Count': og['strain2_count']\n",
    "            })\n",
    "\n",
    "        if shared_data:\n",
    "            df_shared = pd.DataFrame(shared_data)\n",
    "            df_shared.to_excel(writer, sheet_name='Shared_Orthogroups', index=False)\n",
    "            print(f\"Exported {len(shared_data)} shared orthogroups\")\n",
    "\n",
    "        # Sheet 3: ATCC 14067-specific\n",
    "        s1_data = []\n",
    "        for og in results['strain1_only_orthogroups']:\n",
    "            s1_data.append({\n",
    "                'Orthogroup': og['orthogroup'],\n",
    "                'Genes': ', '.join(sorted(og['genes'])),\n",
    "                'Gene_Count': og['count']\n",
    "            })\n",
    "\n",
    "        if s1_data:\n",
    "            df_s1 = pd.DataFrame(s1_data)\n",
    "            df_s1.to_excel(writer, sheet_name='ATCC14067_Specific', index=False)\n",
    "            print(f\"Exported {len(s1_data)} ATCC 14067-specific orthogroups\")\n",
    "\n",
    "        # Sheet 4: ATCC 13032-specific\n",
    "        s2_data = []\n",
    "        for og in results['strain2_only_orthogroups']:\n",
    "            s2_data.append({\n",
    "                'Orthogroup': og['orthogroup'],\n",
    "                'Genes': ', '.join(sorted(og['genes'])),\n",
    "                'Gene_Count': og['count']\n",
    "            })\n",
    "\n",
    "        if s2_data:\n",
    "            df_s2 = pd.DataFrame(s2_data)\n",
    "            df_s2.to_excel(writer, sheet_name='ATCC13032_Specific', index=False)\n",
    "            print(f\"Exported {len(s2_data)} ATCC 13032-specific orthogroups\")\n",
    "\n",
    "    print(f\"\\nAll data exported to: {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    input_file = \"converted_orthogroups.tsv\"\n",
    "    output_dir = \"./orthooutput\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OrthoFinder Comparative Analysis\")\n",
    "    print(\"C. glutamicum ATCC 14067 vs ATCC 13032\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    print(\"Parsing OrthoFinder results...\")\n",
    "    results = parse_orthogroups(input_file, \"14067\", \"13032\")\n",
    "\n",
    "    print(\"\\nGenerating comprehensive report...\")\n",
    "    generate_comprehensive_report(results, os.path.join(output_dir, \"analysis_report.txt\"))\n",
    "\n",
    "    print(\"\\nExporting all data to Excel...\")\n",
    "    export_to_excel(results, os.path.join(output_dir, \"ortholog_data.xlsx\"))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Analysis complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nOutput files:\")\n",
    "    print(\"  1. analysis_report.txt  - Comprehensive text report with statistics and previews\")\n",
    "    print(\"  2. ortholog_data.xlsx   - Excel file with 4 sheets containing all gene data\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "bf8bd5c40701222c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OrthoFinder Comparative Analysis\n",
      "C. glutamicum ATCC 14067 vs ATCC 13032\n",
      "================================================================================\n",
      "\n",
      "Parsing OrthoFinder results...\n",
      "Analyzing: Corynebacterium glutamicum ATCC 14067 vs Corynebacterium glutamicum ATCC 13032\n",
      "\n",
      "Generating comprehensive report...\n",
      "================================================================================\n",
      "ORTHOFINDER COMPARATIVE ANALYSIS REPORT\n",
      "Corynebacterium glutamicum ATCC 14067 vs ATCC 13032\n",
      "================================================================================\n",
      "\n",
      "1. SUMMARY STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                                                       Value\n",
      "--------------------------------------------------------------------------------\n",
      "Total genes in ATCC 14067                                    3,029\n",
      "Total genes in ATCC 13032                                    2,941\n",
      "\n",
      "2. ORTHOGROUP ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "Total orthogroups analyzed:                                  3,306\n",
      "Shared orthogroups (core genome):                            2,570\n",
      "Orthogroups unique to ATCC 14067:                              405\n",
      "Orthogroups unique to ATCC 13032:                              331\n",
      "Shared orthogroups (%):                                      77.7%\n",
      "\n",
      "3. GENE DISTRIBUTION\n",
      "--------------------------------------------------------------------------------\n",
      "Core genes (ATCC 14067):                                     2,599\n",
      "Core genes (ATCC 13032):                                     2,605\n",
      "Strain-specific genes (ATCC 14067):                            430\n",
      "Strain-specific genes (ATCC 13032):                            336\n",
      "\n",
      "4. CORE GENOME ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "Core genome proportion (ATCC 14067):                         85.8%\n",
      "Core genome proportion (ATCC 13032):                         88.6%\n",
      "\n",
      "5. DATA PREVIEW - SHARED ORTHOGROUPS (First 10)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Orthogroup: OG0000000\n",
      "   ATCC 14067 genes (4): CEY17_RS05010, CEY17_RS11195, CEY17_RS16695, CEY17_RS16845\n",
      "   ATCC 13032 genes (3): CGL_RS04650, CGL_RS11310, CGL_RS12090\n",
      "\n",
      "2. Orthogroup: OG0000001\n",
      "   ATCC 14067 genes (2): CEY17_RS03620, CEY17_RS03815\n",
      "   ATCC 13032 genes (2): CGL_RS03290, CGL_RS03475\n",
      "\n",
      "3. Orthogroup: OG0000002\n",
      "   ATCC 14067 genes (2): CEY17_RS16575, CEY17_RS16585\n",
      "   ATCC 13032 genes (2): CGL_RS15650, CGL_RS15655\n",
      "\n",
      "4. Orthogroup: OG0000003\n",
      "   ATCC 14067 genes (1): CEY17_RS15285\n",
      "   ATCC 13032 genes (2): CGL_RS08670, CGL_RS13770\n",
      "\n",
      "5. Orthogroup: OG0000004\n",
      "   ATCC 14067 genes (2): CEY17_RS01025, CEY17_RS04265\n",
      "   ATCC 13032 genes (4): CGL_RS00905, CGL_RS03975, CGL_RS08665, CGL_RS09160\n",
      "\n",
      "6. Orthogroup: OG0000005\n",
      "   ATCC 14067 genes (2): CEY17_RS15160, CEY17_RS15350\n",
      "   ATCC 13032 genes (1): CGL_RS07790\n",
      "\n",
      "7. Orthogroup: OG0000006\n",
      "   ATCC 14067 genes (1): CEY17_RS04980\n",
      "   ATCC 13032 genes (2): CGL_RS04620, CGL_RS10830\n",
      "\n",
      "8. Orthogroup: OG0000007\n",
      "   ATCC 14067 genes (1): CEY17_RS06415\n",
      "   ATCC 13032 genes (2): CGL_RS05860, CGL_RS14815\n",
      "\n",
      "9. Orthogroup: OG0000008\n",
      "   ATCC 14067 genes (1): CEY17_RS16780\n",
      "   ATCC 13032 genes (2): CGL_RS08580, CGL_RS15785\n",
      "\n",
      "10. Orthogroup: OG0000009\n",
      "   ATCC 14067 genes (2): CEY17_RS06060, CEY17_RS12640\n",
      "   ATCC 13032 genes (2): CGL_RS05515, CGL_RS12345\n",
      "\n",
      "6. DATA PREVIEW - STRAIN-SPECIFIC ORTHOGROUPS (First 5 each)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ATCC 14067-specific:\n",
      "   1. OG0000021 (2 genes): CEY17_RS01975, CEY17_RS01985\n",
      "   2. OG0000046 (3 genes): CEY17_RS15190, CEY17_RS15320, CEY17_RS15325\n",
      "   3. OG0000066 (2 genes): CEY17_RS16330, CEY17_RS16335\n",
      "   4. OG0002065 (1 genes): CEY17_RS03610\n",
      "   5. OG0002425 (1 genes): CEY17_RS16980\n",
      "\n",
      "ATCC 13032-specific:\n",
      "   1. OG0000027 (2 genes): CGL_RS15695, CGL_RS15705\n",
      "   2. OG0000043 (1 genes): CGL_RS06760\n",
      "   3. OG0000129 (1 genes): CGL_RS13255\n",
      "   4. OG0002242 (1 genes): CGL_RS13005\n",
      "   5. OG0002370 (1 genes): CGL_RS09905\n",
      "\n",
      "7. OUTPUT FILES\n",
      "--------------------------------------------------------------------------------\n",
      "Complete gene lists exported to: ortholog_data.xlsx\n",
      "  - Sheet 1: Summary Statistics\n",
      "  - Sheet 2: Shared Orthogroups\n",
      "  - Sheet 3: ATCC 14067-specific Orthogroups\n",
      "  - Sheet 4: ATCC 13032-specific Orthogroups\n",
      "\n",
      "================================================================================\n",
      "End of Report\n",
      "================================================================================\n",
      "\n",
      "Report saved to: ./orthooutput\\analysis_report.txt\n",
      "\n",
      "Exporting all data to Excel...\n",
      "Exported 2570 shared orthogroups\n",
      "Exported 405 ATCC 14067-specific orthogroups\n",
      "Exported 331 ATCC 13032-specific orthogroups\n",
      "\n",
      "All data exported to: ./orthooutput\\ortholog_data.xlsx\n",
      "\n",
      "================================================================================\n",
      "Analysis complete!\n",
      "================================================================================\n",
      "\n",
      "Output files:\n",
      "  1. analysis_report.txt  - Comprehensive text report with statistics and previews\n",
      "  2. ortholog_data.xlsx   - Excel file with 4 sheets containing all gene data\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
